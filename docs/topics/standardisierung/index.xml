<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" 
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:media="http://search.yahoo.com/mrss/">
  <channel>
    <title>Standardisierung auf Nicky Reinert</title>
    <link>https://nickyreinert.de/topics/standardisierung/</link>
    <description>Blog &amp; Projekte von Nicky Reinert (Institut für digitale Herausforderungen): Webentwicklung &amp; Software Development, SEO &amp; Analytics, Hosting &amp; DevOps, WordPress &amp; Hugo, Tools &amp; Projekte, Datenschutz und digitale Kultur – plus Texte zu KI sowie Autismus &amp; Gesellschaft.</description>
    <generator>Hugo 0.148.2</generator>
    <language>de</language>
    <managingEditor></managingEditor>
    <webMaster></webMaster>
    <copyright></copyright>
    <lastBuildDate>Mon, 15 Jan 2024 12:19:31 +0100</lastBuildDate><atom:link href="https://nickyreinert.de/topics/standardisierung/index.xml" rel="self" type="application/rss+xml" /><item>
      <title>Die Nerd Enzyklopädie 45 - 80 Spalten und die Legende vom Pferdehintern</title>
      <link>https://nickyreinert.de/2024/2024-01-15-nerd-enzyklop%C3%A4die-45---80-spalten-und-die-legende-vom-pferdehintern/</link>
      <pubDate>Mon, 15 Jan 2024 12:19:31 +0100</pubDate>
      <author></author>
      <guid>https://nickyreinert.de/2024/2024-01-15-nerd-enzyklop%C3%A4die-45---80-spalten-und-die-legende-vom-pferdehintern/</guid>
      <description>Für die einen ist sie unerforschtes Land, für die anderen die linke Westentasche: Die Kommandozeile! Eine Eigenschaft eint die meisten Kommandozeilen-Programme …</description>
      
      
      <content:encoded>&lt;![CDATA[
        
        <div class="ai-summary">
          <h3>AI-Zusammenfassung</h3>
          <p>Eine Erkundung aus der &#39;Nerd Enzyklopädie&#39; über die Ursprünge des 80-Zeichen-Spaltenstandards in der Informatik. Der Artikel beleuchtet historische Einflüsse wie Schreibmaschinen und Lochkarten (insbesondere IBMs 80-Spalten-Lochkarten) und diskutiert die Lesbarkeitsargumente. Humorvoll wird die Verbindung zur &#39;Pferdehintern-Legende&#39; hergestellt, um die oft obskuren Ursprünge technischer Standards zu illustrieren.</p>
          
          
          <p><strong>Hauptthemen:</strong> Computergeschichte, Standardisierung, Programmierung, Typografie, Nerd-Kultur, Humor</p>
          
          
          
          <p><strong>Schwierigkeitsgrad:</strong> beginner</p>
          
        </div>
        
        
        <p>Für die einen ist sie unerforschtes Land, für die anderen die <strong>linke</strong> <strong>Westentasche</strong>: Die <strong>Kommandozeile</strong>! Eine Eigenschaft eint die meisten Kommandozeilen-Programme unter <strong>Windows</strong>, <strong>MacOS</strong> und <strong>Linux</strong>: Sie sind in der Standard-Konfiguration <strong>80 Zeichen</strong> bzw. <strong>Spalten</strong> breit. Dieser Standard begegnet einem auch an anderen Orten, wie in <strong>Code Editoren</strong> oder <strong>Style Guides</strong> von Programmiersprachen, die 80 Zeichen pro Zeile als sinnvollen Standard beschreiben.</p>
<p><img src="/2024/2024-01-15-nerd-enzyklop%C3%A4die-45---80-spalten-und-die-legende-vom-pferdehintern/image_1.png" alt=""></p>
<p>Es gibt viele Erklärungsversuche für dieses Standard-Maß. Der erste liegt in der Lesbarkeit:</p>
<p>Sind die Zeilen zu kurz,
muss man beim Lesen
öfter die Zeile
wechseln, was den
Lesefluss
erschwert.</p>
<p>Sind sie zu lang, ist es sehr schwer, einzelne Zeilen zu unterscheiden.</p>
<p>In der Typographie gelten deshalb <strong>40 bis 90 Zeichen</strong> pro Zeile als sinnvoll, ideal sind mindestens <strong>60 Zeichen</strong> [<a href="https://www.semanticscholar.org/paper/Markus-Itkonen-Typography-and-readability-Itkonen-groteski/4b67cd16136d47682f547619e705e2151d2b98df">SEMAN1</a>]. Das erklärt allerdings noch nicht, warum wir in der elektronischen Datenverarbeitung 80 Zeichen bevorzugen.</p>
<p>Einer weiterer Grund könnte die technische Limitierung alter <strong>Schreibmaschinen</strong> sein. Diese konnten in der Regel nur <strong>70 bis 80 Zeichen</strong> pro Zeile verarbeiten. Hier spielte auch die Breite des Papiers eine wichtige Rolle: Der US-Standard „<strong>Letter</strong>“ erlaubte mit seinem <strong>8,5 x 11 Zoll</strong> in etwa <strong>85 bis 102 Zeichen pro Zeile</strong> bei einer Schriftgröße von <strong>10 bis 12 Punkten</strong>. Berücksichtigt man den linken und rechten Rand, kam man auf <strong>55–78 Zeichen pro Zeile</strong> [<a href="https://en.wikipedia.org/wiki/Characters_per_line">WIKI15</a>].</p>
<p>Und dann gibt es noch diese Erklärung: Das 80-Zeichen-Limit stammte von den Terminals der <strong>1960er</strong> und <strong>1970er</strong> Jahre. Dort nutze man ebenfalls <strong>80 Spalten bei 24 bis 25 Zeilen</strong>. Damit orientierte man sich an den damals üblichen <strong>Lochkarten</strong>. Lochkarten waren zu dieser Zeit weit verbreitete Datenträger. Sie enthielten auf einer Zeile bis zu <strong>80 Löcher</strong>. Zwar gab es auch Lochkarten mit mehr oder weniger Spalten. Aber <strong>IBM</strong>, damals schon ein Big-Player, hatte <strong>80-Spalten-Lochkarten</strong> im Angebot und diese Konfiguration sozusagen als Standard etabliert [<a href="https://en.wikipedia.org/wiki/Punched_card">WIKI16</a>].</p>
<p>Warum IBM ausgerechnet 80 Löcher pro Zeile wählte, ist nicht sicher belegt. Einerseits aus den oben schon genannten ergonomischen Gründen, sicher aber auch aus technischen Gründen: So konnten genug Löcher mit einer sinnvollen Größe auf dem Standard-Papier untergebracht werden.</p>
<p>Mehr Löcher würden die Integrität der Lochkarten stören, kleinere Löchere wären anfälliger für Fehler und weniger und größere Löcher wären unwirtschaftlich. Außerdem basierten die IBM-Lochkarten auf einer weitaus älteren Maschine: <strong>Holleriths Tabulatormaschine</strong>. <strong>Herman Hollerith</strong> erfand die Tabulatormaschine um <strong>1890</strong>, um die US Volkszählung zu unterstützen.</p>
<p>Ab hier geht die Geschichte sehr verworrene Wege: Holleriths Lochkarten könnten auf den Lochkarten basieren, die <strong>Basile Bouchon</strong> um <strong>1725</strong> erfand, um seinen „<strong>automatischen Webstuhl</strong>“ zu steuern; damals noch einfaches perforiertes Papier. <strong>Bouchon</strong> hat sich für seine Entwicklung vermutlich am Standard für Banknoten orientiert: Um <strong>1400</strong> nutze man das Papierformat “<strong>British Imperial Foolscap</strong>“, um Banknoten zu drucken: Auf einer Seite wurden <strong>8 Banknoten</strong> in <strong>2 Spalten</strong> angeordnet. Man könnte nun noch etwas weiter gehen: Die Breite des Papiers lässt sich auf den Herstellungsprozess zurückführen: Die Spannweite der Arme der Arbeiter:innen während der Papierherstellung gibt ein natürliches Maß für die maximale Breite der Papierbögen vor.</p>
<p>Das Standardmaß „80 Zeichen pro Seite“ kann also, mit etwas Fantasie und Augenzwinkern, also auf die Spannbreite der Arme des Menschen zurückgeführt werden. Und das erinnert stark an die Herleitung der Durchmesser von <strong>Glasflaschen</strong>, der sich über <strong>Europaletten</strong>, die <strong>Spurbreite</strong> von <strong>Schienen</strong> schließlich auf die <strong>Breite von Pferdehintern</strong> zurückzuführen lässt: die sogenannte <strong>Pferdehintern-Geschichte</strong> (<strong>Horse Ass Story</strong>) — bei der es sich übrigens auch nur um eine urbane Legende handelt [<a href="https://parovoz.com/spravka/standardgauge-de.html">PARO1</a>].</p>
<p>Nachdem wir uns nun mühsam durch mehr als 600 Jahre Geschichte gekämpft haben, kommt ein klitzekleiner Dämpfer: Auch wenn <strong>80 Zeichen pro Zeile</strong> weit verbreitet sind, werden mitunter auch <strong>72, 79, 100, 132</strong> oder gar <strong>180</strong> Zeichen pro Zeile gerne als Standard genutzt [<a href="https://en.wikipedia.org/wiki/Characters_per_line">WIKI18</a>]. Und vermutlich versteckt sich hinter jeder dieser Angaben eine kleine Pferdehintern-Legende…</p>

        
        
        <div class="tags">
          <p><strong>Tags:</strong> IT-Geschichte, Standard, Nerd-Enzyklopädie, Humor</p>
        </div>
        
      ]]></content:encoded>
      
      
      
      <category>nerdenz</category>
      
      
      
      
      <media:content url="https://nickyreinert.de/images/posts/placeholder.jpg" type="image/jpeg">
        <media:title>Die Nerd Enzyklopädie 45 - 80 Spalten und die Legende vom Pferdehintern - Titelbild</media:title>
      </media:content>
      
      
      
      
      <dc:subject>Lesezeit: 5 Minuten</dc:subject>
      
      
      
      <dc:type>historical_anecdote</dc:type>
      
      
    </item><item>
      <title>Die Nerd Enzyklopädie - Warum ist 1 Byte 8 Bit groß?</title>
      <link>https://nickyreinert.de/2022/2022-01-15-die-nerd-enzyklop%C3%A4die---warum-ist-1-byte-8-bit-gro%C3%9F/</link>
      <pubDate>Mon, 30 Jan 2023 12:19:31 +0100</pubDate>
      <author></author>
      <guid>https://nickyreinert.de/2022/2022-01-15-die-nerd-enzyklop%C3%A4die---warum-ist-1-byte-8-bit-gro%C3%9F/</guid>
      <description>Die Einführung in das Binärsystem dürfte Gegenstand jeder Informatik-Vorlesung sein und schon für viele verzweifelte Gesichter gesorgt haben. Sei es drum: Um …</description>
      
      
      <content:encoded>&lt;![CDATA[
        
        <div class="ai-summary">
          <h3>AI-Zusammenfassung</h3>
          <p>Eine detaillierte historische und technische Erklärung, warum ein Byte heute standardmäßig 8 Bit groß ist. Der Artikel beleuchtet die Ursprünge des Begriffs &#39;Byte&#39;, die Entwicklung von Bit-Größen in frühen Computersystemen, die Rolle von ASCII und IBM bei der Etablierung des 8-Bit-Standards und die spätere offizielle Normierung sowie die Einführung des Begriffs &#39;Oktett&#39;.</p>
          
          
          <p><strong>Hauptthemen:</strong> Computer Science, Informatik, Hardware, Datenrepräsentation, Geschichte der IT, Standardisierung</p>
          
          
          
          <p><strong>Schwierigkeitsgrad:</strong> beginner</p>
          
        </div>
        
        
        <p>Die Einführung in das Binärsystem dürfte Gegenstand jeder Informatik-Vorlesung sein und schon für viele verzweifelte Gesichter gesorgt haben. Sei es drum: Um einen kleinen Ausflug in das Binär-System kommen wir nicht herum, wenn wir die Frage nach dem Byte klären wollen. Und da uns das binäre Zahlensystem noch öfter begegnen wird, steht dieses Kapitel ganz am Anfang.</p>
<p>Der Begriff Byte kam bereits 1956 auf und wurde von Werner Buchholz geprägt. Buchholz arbeitete für IBM an dem Supercomputer IBM 7030,  Projektname &ldquo;Stretch&rdquo;. In einem Konzeptpapier beschrieb er den Einsatz von &ldquo;characters, or &lsquo;bytes&rsquo; as we have called them&rdquo;. Buchholz leitete Byte vom englischen &ldquo;bite&rdquo; für &ldquo;der Bissen&rdquo; ab und wählte die Schreibweise mit dem Y, um eine Verwechselung mit  dem Bit zu vermeiden. Er definierte Bytes damals als Folge von 2 bis 6 Bits [<a href="https://blog.hnf.de/bitte-ein-byte/">HNF1</a>].</p>
<p>Das Bit ist ein Kofferwort aus <em><strong>b</strong>inary</em> und <em>dig<strong>it</strong>.</em> - zweiwertige Ziffer. 0 und 1. Bit ist aber auch ein englisches Wort und heißt übersetzt &ldquo;das Bisschen&rdquo;. Es geht hier also um Bisschen und Bissen. Guten Appetit.</p>
<p>Der begriffliche Vorgänger des Bytes nennt sich übrigens Syllable, kurz Slab. Aber auch die Slabs unterwarfen sich keiner einheitlichen Ordnung. Der Computer der Saturn V Rakete arbeitet z.B. mit 13 Bit großen Syllables.</p>
<p>Aber zurück zu den Bits und Bytes und der Frage, warum 1 Byte genau 8 Bit groß ist.</p>
<p>Ein Computer versteht genau zwei Zustände: Entweder fließt ein Strom oder es fließt  kein Strom. Aus oder An (damit ignorieren wir übrigens komplett die Architektur der sogenannten Quanten-Computer).</p>
<p>Mithilfe dieses Prinzips kann ein Computer Informationen speichern, verarbeiten und wiedergeben. Es handelt sich um die kleinste Informationseinheit: 1 Bit. Stellen wir uns einfach vor, dass es sich hierbei um kleine Lämpchen handelt, die entweder an oder aus sind und diesen Zustand beschreiben wir mit 0 für aus und 1 für an. Hier sind 8 Lämpchen nebeneinander:</p>
<pre><code>0000 0000
</code></pre>
<p>So sieht es aus, wenn das kleine Lämpchen ganz rechts an ist:</p>
<pre><code>0000 0001
</code></pre>
<p>Und daraus kann man nun einen Zahlenwert ablesen, indem man Potenzen bildet - und das ist ein Stück faszinierende Mathematik. Da es genau zwei Zustände gibt (aus oder an), ist die Basis der Potenz 2. Die Position des aktivierten Lämpchens dient als Exponent. Gezählt wird von rechts nach links und wir fangen bei 0 an - 0 ist ja auch ein Wert, wenngleich kein großer.</p>
<p>Damit lautet die Rechnung  2^0 und das ist 1! Die 1 ganz rechts steht für den Wert 1. Ok, das war noch einfach. Was ist mit der 2?</p>
<pre><code>0000 0010
</code></pre>
<p>Nun ist das Lämpchen an der 1. Position an. 2^1^. Oder: 2. Das mit den Einsen und Nulle ist eigentlich gar nicht so schwer, oder? Manchmal frage ich mich, warum wir uns überhaupt mit dem Dezimalsystem abmühen.</p>
<p>Jetzt bist du dran. Welchen Wert sehen wir hier?</p>
<pre><code>0000 0011
</code></pre>
<p>Korrekt: 3. Warum? Man summiert die Werte der einzelnen Positionen:</p>
<pre><code>2¹ + 2⁰ = 2 + 1 = 3
</code></pre>
<p>Bravo. Du hast das binäre System verstanden. Je mehr Lämpchen leuchten, desto größere Zahlen können wir abbilden. Die kleinen Lämpchen im Computer geben natürlich kein sichtbares Licht ab, dafür aber Wärme. Jetzt weißt du, warum dein Handy manchmal so heiß wird.</p>
<p>Mit 8 Bit lassen sich z.B. Werte bis zu 2⁸ - 1 verarbeiten. Warum -1? Da man die Positionen von 0 anfängt zu zählen, ist die größte Position 7.  Das ist also der größte Exponent. Sind also alle Bits aktiviert (auf 1 gesetzt), berechnet man den Wert folgendermaßen:</p>
<pre><code>128 + 64 + 32 + 16 + 8 + 4 + 2 + 1 = 255
</code></pre>
<p>Und für die 255 gibt es eine kleine Abkürzung: Das ist nämlich nichts anderes als 2⁸ - 1.</p>
<p>Den höchsten Wert darf man allerdings nicht mit der Anzahl möglicher Werte verwechseln. Die 0 ist auch Teil der kleinen Familie. Damit gibt es 256 unterschiedliche Werte.</p>
<p>Nach dieser rührenden Familienzusammenführung gebe ich mit der folgenden Frage zurück ins Hauptstadtstudio: Warum hat 1 Byte denn nun ausgerechnet 8 Bit?</p>
<p>Wir können mit den Bits zwar beliebige Zahlen abbilden, aber wie sieht es mit  Buchstaben aus? Die Lösung ist so fantastisch simpel: Wir legen eine Tabelle an, eine Zeichentabelle, auch Codetabelle genannt. Jeder Buchstabe wird durch eine Zahl repräsentiert:</p>
<pre><code>1 A
2 B
3 C
...
</code></pre>
<p>Die minimale Größe der Tabelle ist zunächst naheliegend: Wir brauchen mindestens 26 Einträge. Um ein einfaches Alphabet abbilden zu können, muss der Speicher also mindestens 5 Bit groß sein:</p>
<pre><code>2⁵ = 32 Einträge
</code></pre>
<p>(Warum nicht 2⁵ - 1 = 31 Einträge? Weil die 0 in einer Code-Tabelle theoretisch auch als Verweis dienen kann, denk an die Familie!)</p>
<p>5 Bit entspricht der Größe der ersten Codetabelle mit dem Namen Baudot-Code bzw. Baudot-Murray-Code. Auf der 0. Position befand sich tatsächlich nichts. Die Position 1 (00001) verweist auf Buchstaben E &mdash;weil das der häufigste Buchstabe ist. A wird mit der 3 codiert (00011) und so weiter. </p>
<p>Das erklärt aber immer noch nicht warum 1 Byte genau 8 Bit groß ist. Also weiter in der Geschichte:</p>
<p>Der Baudot-Murray-Code hatte für die elektronische Datenverarbeitung noch keine große Bedeutung. Man konnte nicht zwischen Groß- und Kleinschreibung unterscheiden, die Zahlen 0 bis 9 waren nicht darstellbar  und was ist mit den ganzen Satzzeichen? Man brauchte eigentlich eine Tabelle mit mindestens 26 + 26 + 10, also 62 Einträgen.</p>
<p>Das Militär entwickelte 1956 im Rahmen des FIELDATA-Projekts eine Zeichentabelle mit 6 Bit, also 64 Einträgen, die auch als Basis für den UNIVAC 1100 diente [<a href="https://de.wikipedia.org/wiki/Fieldata">WIKI9</a>]. Diese Tabelle ermöglichte zwar keine Kleinschreibung, dafür aber Satzzeichen,  Ziffern und Steuerzeichen.
Wozu Steuerzeichen? Da eine Zeichentabelle als Grundlage zur Darstellung auf den Monitor dient, muss sie auch Anweisungen wie z.B. Leerzeichen, Entfernen und so weiter enthalten.</p>
<p>Für den wirklich praktischen Gebrauch kamen also nur 7 Bit infrage. Und genau das ist auch die Länge der berühmt-berüchtigten ASCII-Codetabelle von 1963, die uns noch eine ganze Weile begleiteten wird. Um nicht zu sagen: ASCI wird dir auch in 2023 noch begegnen.</p>
<p>Aus den 7 Bits für ein sinnvollen Zeichensatz wurden recht schnell 8 Bit und zwar aus vielen Gründen: Die 8 ist ein vielfaches von 2. Und das ist in einem binären System viel schöner als die 7. Außerdem konnte man das zusätzliche Bit für andere Funktionen nutzen, wie zB. zur Fehlerkontrolle (&ldquo;Parity-Bit”) oder um den Umfang der Tabelle zu erhöhen. Und dann kam auch noch IBM um die Ecke und brachte 1981 den allerersten und extrem erfolgreichen Personal Computer (PC) auf den Markt. IBM stattete diesen mit einer 8 Bit-Zeichentabelle aus. Der Name: Code Page 437. In den folgenden Jahren hat sich so die Größe von 8 Bit für 1 Byte als Quasi-Standard etabliert.</p>
<p>Tatsächlich konnte ein Byte aber alles sein: Angefangen bei einem 1 Bit bis zu dem, was Mitte des 20. Jahrhunderts als Speicher so verfügbar war: 6 Bit, 9 Bit, 10 Bit.  Beim Nixdorf 820 betrug die Größe eines Bytes 12 Bit. Es gibt Systeme, bei denen die Byte-Größe sogar frei wählbar war und immer noch ist. Der PDP-10 erlaubte eine wählbare Größe zwischen 1 und 36 Bit [<a href="https://de.wikipedia.org/wiki/Byte">WIKI10</a>].</p>
<p>Die traurige Wahrheit lautet also: Es gab sehr lange gar keine einheitliche Definition für die Größe von 1 Byte. Anfang der 1990er Jahre, als Computer begannen die privaten Haushalte zu erobern, rang man sich dann doch noch zu einem offiziellen Standard durch: In der ISO/IEC 2382-1:1993 wurde die Byte-Größe mit 8 Bit definiert.</p>
<p>Allerdings halten sich auch heute nicht alle ans diesen Standard. Viele Programmiersprachen erlauben die Anzahl von Bits individuell festzulegen. Um das Byte mit seiner unsteten Beziehung zu den Bits zu unterscheiden, wurde mit dem Standard IEC 60027-2 in 1999 die Bezeichnung Oktet eingeführt. Ein Oktet entspricht immer 8 Bits.</p>
<p>Wenn du also auf Nummer Sicher gehen willst, solltest du dich nicht darauf verlassen, dass 1 Byte genau 8 Bit enthält.</p>

        
        
        <div class="tags">
          <p><strong>Tags:</strong> Informatik, Geschichte, Hardware, Nerd-Enzyklopädie</p>
        </div>
        
      ]]></content:encoded>
      
      
      
      <category>nerdenz</category>
      
      
      
      
      <media:content url="https://nickyreinert.de/images/posts/placeholder.jpg" type="image/jpeg">
        <media:title>Die Nerd Enzyklopädie - Warum ist 1 Byte 8 Bit groß? - Titelbild</media:title>
      </media:content>
      
      
      
      
      <dc:subject>Lesezeit: 5 Minuten</dc:subject>
      
      
      
      <dc:type>technical_explainer</dc:type>
      
      
    </item>
  </channel>
</rss>