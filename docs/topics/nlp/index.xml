<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" 
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:media="http://search.yahoo.com/mrss/">
  <channel>
    <title>NLP auf Nicky Reinert</title>
    <link>https://nickyreinert.de/topics/nlp/</link>
    <description>Blog &amp; Projekte von Nicky Reinert (Institut für digitale Herausforderungen): Webentwicklung &amp; Software Development, SEO &amp; Analytics, Hosting &amp; DevOps, WordPress &amp; Hugo, Tools &amp; Projekte, Datenschutz und digitale Kultur – plus Texte zu KI sowie Autismus &amp; Gesellschaft.</description>
    <generator>Hugo 0.148.2</generator>
    <language>de</language>
    <managingEditor></managingEditor>
    <webMaster></webMaster>
    <copyright></copyright>
    <lastBuildDate>Fri, 21 Oct 2022 07:51:37 +0100</lastBuildDate><atom:link href="https://nickyreinert.de/topics/nlp/index.xml" rel="self" type="application/rss+xml" /><item>
      <title>Random Knowledge</title>
      <link>https://nickyreinert.de/2022/2022-10-21-random-knowledge/</link>
      <pubDate>Fri, 21 Oct 2022 07:51:37 +0100</pubDate>
      <author></author>
      <guid>https://nickyreinert.de/2022/2022-10-21-random-knowledge/</guid>
      <description>&ldquo;Random Knowledge&rdquo; ist ein automatisierter Podcast, bei dem eine computer-generierte Stimme zufällige Artikel der Wikipedia vorliest.
Dazu wird in …</description>
      
      
      <content:encoded>&lt;![CDATA[
        
        <div class="ai-summary">
          <h3>AI-Zusammenfassung</h3>
          <p>Eine Vorstellung des Projekts &#39;Random Knowledge&#39;, eines automatisierten Podcasts, der zufällige englische Wikipedia-Artikel mit einer computergenerierten Stimme vorliest. Der Artikel beschreibt den technischen Workflow, von der Textaufbereitung über die Umwandlung in Sprache mittels Google Text-to-Speech bis zur Distribution über &#39;anchor.fm&#39; an verschiedene Podcast-Plattformen.</p>
          
          
          <p><strong>Hauptthemen:</strong> Podcast, Automatisierung, Text-to-Speech, Wikipedia, Python, NLP, KI-Anwendung</p>
          
          
          
          <p><strong>Schwierigkeitsgrad:</strong> intermediate</p>
          
        </div>
        
        
        <p>&ldquo;Random Knowledge&rdquo; ist ein automatisierter Podcast, bei dem eine computer-generierte Stimme zufällige Artikel der Wikipedia vorliest.</p>
<p>Dazu wird in Python ein zufälliger Artikel der englischen Wikipedia abgerufen und vorbereitet. Der gesamte Artikel wird in Abschnitte getrennt, Bereiche, die nicht vorlesbar sind, wie z.B. Tabellen, werden entfernt. Über die Text-to-Speech-API von Google wird der Text in Sprache umgewandelt und als Audio-Datei abgelegt. Die Dateien werden über eine undokumentierte Schnittstelle zu anchor.fm hochgeladen und von dort an die gängigen Portale verteilt (Spotify, Deezer, Google, Amazon, Apple, &hellip;)</p>
<p><a href="https://spotifyanchor-web.app.link/e/zjG3R1ANFxb">https://spotifyanchor-web.app.link/e/zjG3R1ANFxb</a></p>

        
        
        <div class="tags">
          <p><strong>Tags:</strong> Podcast, Automatisierung, KI, Python, Projekt</p>
        </div>
        
      ]]></content:encoded>
      
      
      
      <category>projekte</category>
      
      
      
      
      <media:content url="https://nickyreinert.de/images/posts/placeholder.jpg" type="image/jpeg">
        <media:title>Random Knowledge - Titelbild</media:title>
      </media:content>
      
      
      
      
      <dc:subject>Lesezeit: 5 Minuten</dc:subject>
      
      
      
      <dc:type>project_showcase</dc:type>
      
      
    </item><item>
      <title>Augenblicke - Eine statistische Analyse des Flirt-Portals der BVG</title>
      <link>https://nickyreinert.de/2021/2021-01-21-augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/</link>
      <pubDate>Thu, 21 Jan 2021 00:00:00 +0000</pubDate>
      <author></author>
      <guid>https://nickyreinert.de/2021/2021-01-21-augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/</guid>
      <description>Was ist &ldquo;Augenblicke&rdquo;? Im Frühjahr 2006, mehr als 6 Jahre bevor Tinder die Herzen der Smarthphone-Besitzer im Sturm eroberte, startete die BVG auf …</description>
      
      
      <content:encoded>&lt;![CDATA[
        
        <div class="ai-summary">
          <h3>AI-Zusammenfassung</h3>
          <p>Dieser Artikel präsentiert eine statistische Analyse von &#39;Augenblicke&#39;, dem Flirt-Portal der Berliner Verkehrsbetriebe (BVG). Er untersucht die Entwicklung der Beitragszahlen, die Nutzung verschiedener Linien und Verkehrsmittel, die Popularität von Pseudonymen sowie zeitliche Muster und die Stimmung der Nachrichten. Zudem werden die Methodik der Datenerfassung und mögliche Fehlerquellen detailliert beleuchtet.</p>
          
          
          <p><strong>Hauptthemen:</strong> Datenanalyse, Statistik, BVG, Berlin, Öffentlicher Nahverkehr, Flirt-Portale, Web Scraping, NLP, Sentiment Analyse</p>
          
          
          
          <p><strong>Schwierigkeitsgrad:</strong> advanced</p>
          
        </div>
        
        
        <h2 id="was-ist-augenblicke">Was ist &ldquo;Augenblicke&rdquo;?</h2>
<p>Im Frühjahr 2006, mehr als 6 Jahre bevor <strong>Tinder</strong> die Herzen der Smarthphone-Besitzer im Sturm eroberte, startete die BVG <a href="https://www.bvg.de/de/Meine-BVG/Meine-Augenblicke/Alle-Augenblicke">auf ihrer Seite ein Portal mit dem Namen &ldquo;<strong>Augenblicke</strong>&rdquo;</a>. Im Gegensatz zu Tinder sind sich die Nutzer von <em>Augenblicke</em> in der Regel aber ziemlich sicher schon einmal über den Weg gelaufen - nämlich in einer der zahlreichen Fahrzeuge der BVG oder S-Bahn.</p>
<p><em><strong>Augenblicke</strong></em> will diese verlorenen Seelen nun zusammenbringen. Wer beim ersten Treffen nicht den Mut aufgebracht hat, das Gegenüber anzusprechen, darf das später mit einer Nachricht auf dem Portal nachholen. Das geschieht unter der Angabe eines <strong>Pseudonyms</strong>, <strong>ein paar Zeilen Text</strong>, <strong>der Tram-, Bus- oder Bahn-Linie</strong> und natürlich dem <strong>Zeitpunkt</strong>, wann man sich über den Weg gelaufen ist - der sogenannte und namensgebende <strong>Augenblick</strong>. Das Prinzip ist also recht einfach. Nach einem verhaltenen Start Anfang 2006 dauerte es erstmal einige Zeit, bis sich ein gewisser Erfolg zeigte.</p>
<p><em>(Hinweise zur Methodik und Fehlerquellen am Ende)</em></p>
<h2 id="kleinermannmitbart">KleinerMannMitBart</h2>
<p><a href="https://www.bvg.de/de/Meine-BVG/Meine-Augenblicke/Alle-Augenblicke?act=read-moment&amp;id=2846">Zum allerersten Augenblick</a> kam es an einem <strong>Valentinstag</strong>: Am 14. Februar 2006 um 5 Uhr traf es <strong>KleinerMannMitBart</strong> in der <strong>Buslinie 284</strong>:</p>
<blockquote>
<p>&ldquo;Du (blond, grüne Augen, schlank)&rdquo;</p>
<p>KleinerMannMitBart, 14. Februar 2006</p></blockquote>
<p>Der Dienst dümpelte dann eine Weile vor sich hin, bevor er in der Öffentlichkeit nachhaltig wahrgenommen wird. Erst in 2007 nahm das Portal tatsächlich <em>fahrt</em> auf: die monatliche Beitragszahl stieg auf über 100. Der vorläufige Höhepunkt wurde <strong>im Mai 2018 mit 291 Beiträgen</strong> im Monat erreicht, fast 10 Gesuche pro Tag! Doch dann war der Hype schon wieder vorbei, das <strong>Ende der fetten Flirtjahre</strong>:</p>
<p><a href="https://www.nickyreinert.de/files/augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/grafik-21.png"><img src="/2021/2021-01-21-augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/images/grafik-21-700x435.png" alt=""></a></p>
<p>Anzahl Beiträge / Jahr nach Anbieter</p>
<p>Der Abwärtstrend ist insofern überraschend, da die Zahl der tatsächlichen Fahrgäste in den letzten Jahren stetig zunimmt.</p>
<p>Bis Januar 2021 haben die Besucher in 240 verschiedenen <strong>Linien der BVG, S-Bahn und sogar Deutschen Bahn (Regio!)</strong> <strong>genau 20.108 Augenblicke</strong> erlebt. Zu den meisten Begegnungen kommt es den <strong>U-Bahnen und S-Bahnen</strong>:</p>
<p><a href="https://www.nickyreinert.de/files/augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/grafik-2.png"><img src="/2021/2021-01-21-augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/images/grafik-2-700x367.png" alt=""></a></p>
<p>Wenn man etwas mehr ins Detail geht, zeigt sich, dass die <strong>U-Bahn</strong> mit <strong>8.149 Einträgen</strong> leicht vorne liegt, dicht gefolgt von der <strong>S-Bahn</strong> (<strong>6.581</strong>)</p>
<p><a href="https://www.nickyreinert.de/files/augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/grafik-3.png"><img src="/2021/2021-01-21-augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/images/grafik-3-700x803.png" alt=""></a></p>
<p>Für dieses Ranking kann es übrigens drei Erklärungen geben:</p>
<ol>
<li>Entweder gibt es mehr Singles, die bevorzugt die U-Bahn benutzen, oder</li>
<li>die Fahrt unter der Erde ermutigt die Menschen eher zum Flirten.</li>
<li>Oder die Menschen sind in der U-Bahn besonders schüchtern und müssen daher vermehrt auf dieses Portal zurückgreifen.</li>
</ol>
<h2 id="trakl-und-2151-anonyme">Trakl und 2.151 Anonyme</h2>
<blockquote>
<p>[&hellip;]
Ihr Lächeln, mal amüsiert, gelegentlich auch erstaunt, aber immer von feinster Anmut - ach, es macht(e)mich zufrieden. Nun, die hellen Tage [&hellip;]</p>
<p>Takl am 21. März 2011 in der M1</p></blockquote>
<p><em>Trakl</em> ist nicht nur der Name eines expressionistischen Dichters aus Österreich. Der Name taucht auch auf der Plattform auf und ist mit <strong>39 Einträgen</strong> das am meisten genutzte <strong>Synonym</strong>. Insgesamt wurden <strong>12.298 verschiedene Pseudonyme</strong> genutzt. <strong>2.151 Benutzer</strong> <strong>haben keinen Namen</strong> angeben. Das ist die Top-10 der beliebtesten Namen auf dem Portal:</p>
<p><a href="https://www.nickyreinert.de/files/augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/grafik-4.png"><img src="/2021/2021-01-21-augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/images/grafik-4-700x210.png" alt=""></a></p>
<p>Schaut man sich an, auf welchen Linien <em>die Autoren:innen</em> unterwegs waren, kommt schnell der Verdacht auf, dass es sich jeweils um ein und dieselbe Personen handelt.</p>
<p>Natürlich lässt der Datensatz auch Rückschlüsse auf die <strong>genau Uhrzeit</strong> oder die <strong>exakte</strong> <strong>Linienbezeichnung</strong> und damit vermutlich sogar eine <strong>Pendelstrecke</strong> zu. <em>Aus Gründen</em> möchte ich diese Details hier nicht weiter vertiefen.</p>
<p><a href="https://www.nickyreinert.de/files/augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/grafik-6.png"><img src="/2021/2021-01-21-augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/images/grafik-6-700x184.png" alt=""></a></p>
<h2 id="tageszeiten-und-wochentage">Tageszeiten und Wochentage</h2>
<p>Zurück zur grauen Masse und der Frage: <strong>Zu welcher Tageszeit</strong> und an welchem Wochentag sind die Portal-Nutzer am aktivsten? Zunächst zum offensichtlichen: Die Pendlerzeiten liegen zwischen 7 und 9 Uhr sowie 16 und 19 Uhr. An den Werktagen zeigt sich, dass die Bereitschaft zu Flirten am müden Morgen noch relativ gering ist. Abends, zum Feierabendverkehr - kommt es dagegen zu sehr vielen Kontakten.</p>
<p><a href="https://www.nickyreinert.de/files/augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/grafik-7.png"><img src="/2021/2021-01-21-augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/images/grafik-7-700x768.png" alt=""></a></p>
<p>Beiträge je Wochentag und Tageszeit</p>
<p>Wenig überraschend sieht es am Wochenende etwas anders aus. Hier verlagert sich die Anzahl der Einträge zunächst auf den frühen Nachmittag.</p>
<p>Ein kurzer Blick auf die Stimmung der Beiträge: Eine klare Tendenz lässt sich hier nicht erkennen. In den frühen Morgenstunden scheint die Stimmung stärker zu schwanken als Abends.</p>
<p><a href="https://www.nickyreinert.de/files/augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/grafik-9.png"><img src="/2021/2021-01-21-augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/images/grafik-9-700x771.png" alt=""></a></p>
<p>Stimmung im Wochenverlauf (-1: negativ, 1: positiv)</p>
<h2 id="textanalyse">Textanalyse</h2>
<p>Zunächst ein grober Überblick über die Stimmung im Verlauf der Jahre sowie die durchschnittliche Länge der Beiträge und Wortanzahl.</p>
<p>Die Wortlänge über alle Nachrichten hat sich im Laufe der Jahre kaum verändert. Die Ausschläge zu Beginn der Messung sind auf die geringe Fallzahl zurückzuführen. Danach sind es zwischen 70 und 80 Wörter pro Nachricht. Ab Ende 2014 gibt es einen kurzlebigen Aufwärtstrend in Richtung 90 Wörter pro Eintrag.</p>
<p><a href="https://www.nickyreinert.de/files/augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/grafik-10.png"><img src="/2021/2021-01-21-augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/images/grafik-10-700x364.png" alt=""></a></p>
<p>Zwischen 2011 und 2014 lässt sich ein kleines Stimmungstief erkennen. In 2020 gibt es noch mal einen deutlichen Knick - Auswirkungen von Corona? Auch die (durchschnittliche) Länge der Nachrichten und die Wortanzahl scheint ab 2015 leicht zu steigen.</p>
<p>Die nächste Abbildung zeigt die Stimmung aller Autoren:innen sowie die Objektivität ihrer Nachrichten. Die Objektivität liefert kaum Erkenntnisgewinn und wird hier nur einmal erwähnt. Interesssanter ist die Stimmung, die in den allermeisten Fällen positiv ist, mitunter sogar stark positiv.</p>
<p><a href="https://www.nickyreinert.de/files/augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/grafik-11.png"><img src="/2021/2021-01-21-augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/images/grafik-11-700x435.png" alt=""></a></p>
<p>Stimmung und Objektivität aller Autoren</p>
<p>Weiter geht es mit der Stimmung nach Fahrzeugklasse. Die Abbildung zeigt neben der Stimmung auch die Anzahl der Beiträge (blaue Punkte). So dürfte sich die relativ hohe mittlere Stimmung bei den Nachtbussen erklären. Insgesamt lässt sich vielleicht festhalten, dass die Stimmung in den S- und U-Bahnen sowie Regionalzügen am geringsten ist.</p>
<p><a href="https://www.nickyreinert.de/files/augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/grafik-13.png"><img src="/2021/2021-01-21-augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/images/grafik-13-700x562.png" alt=""></a></p>
<p>Das passt zu der Erkenntnis oben, dass Beiträge mit Bezug zu z.B. Bussen vergleichsweise selten vorkomen: <strong>Sind die Menschen dort glücklicher und demnach vor Ort kontaktfreudiger</strong>?</p>
<p>Die <strong>längsten Nachrichten</strong> mit den <strong>meisten Wörtern</strong> kommen übrigens nicht aus den Linien-Favoriten S-Bahn oder U-Bahn. Nein, es sind Regionalbahnen und die Nacht-Busse, die sich offenbar äußerst fruchtbar auf die Fantasie der Fahrgäste auswirken. Eine Erklärung: Hat man hier mehr Zeit zum Schreiben?</p>
<p><a href="https://www.nickyreinert.de/files/augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/grafik-15.png"><img src="/2021/2021-01-21-augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/images/grafik-15-700x463.png" alt=""></a></p>
<p>Noch mal zurück zur Auswertung der Stimmung. Für das folgende Histogramm habe ich die Stimmungswerte in 0,05 Schritten geclustert um deutlich zu machen, in welchem Bereichen sich die Stimmung der meisten Nachrichten bevorzugt zeigt. Hier ist ganz klar eine Dominanz im neutralen Bereich (0) bis hin zu mittelmäßig positiv (0,5) zu erkennen.</p>
<p><a href="https://www.nickyreinert.de/files/augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/grafik-16.png"><img src="/2021/2021-01-21-augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/images/grafik-16-700x454.png" alt=""></a></p>
<p>Lässt man sich die Cluster im Tagesverlauf anzeigen, dass die Stimmung fast gleichmäßig verteilt zu sein scheint. Nur am Freitag gibt es im Bereich um 0,2 eine höhere Konzentration an Nachrichten (je dunkler die Farbe, desto mehr Nachrichten in diesem Cluster):</p>
<p><a href="https://www.nickyreinert.de/files/augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/grafik-18.png"><img src="/2021/2021-01-21-augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/images/grafik-18-700x556.png" alt=""></a></p>
<h2 id="phrasen-und-wörter">Phrasen und Wörter</h2>
<p>Zum Abschluss noch ein kleiner Blick auf beliebte Phrasen und Wörter. Zunächst alle Wörter (die mindestens 1.000 mal gezählt wurden):</p>
<p><a href="https://www.nickyreinert.de/files/augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/grafik-19.png"><img src="/2021/2021-01-21-augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/images/grafik-19-700x285.png" alt=""></a></p>
<p>Wenig überraschend sind hier beschreibende Wörter wie <strong>Auge, Haar, Jacke</strong>, <strong>Braune</strong>, <strong>Rucksack</strong> oder <strong>Mantel</strong> zu finden. Interessant auch: Das Wort <strong>leider</strong> wird über <strong>6.000</strong> mal verwendet. Nachvollziehbar: Die Plattform ist ja eine Anlaufstelle, für verpasste Chancen. Das absolut häufigste Wort ist <strong>haben</strong> mit über 22.000 Vorkommen. Zur Erklärung: Da die Wortstämme gezählt werden, fallen darunter auch <strong>hat, hast, habe</strong> usw. Sprich: <strong>Ich habe dich gesehen, hast du mich gesehen</strong>.</p>
<p>Nun noch ein kurzer Blick auf die beliebtesten Phrasen:</p>
<p><a href="https://www.nickyreinert.de/files/augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/grafik-20.png"><img src="/2021/2021-01-21-augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/images/grafik-20-700x315.png" alt=""></a></p>
<p>Sehr schön gefällt mir hier das Vorkommen von &ldquo;<strong>unsere Blicke</strong>&rdquo; - was sich nicht nur wunderbar auf den Namen des Portals - Augenblicke - sondern auch seinen Zweck beziehen lässts. Ansonsten finden sich hier natürlich auch naheliegende, beschreibende Phrasen. Es geht ja um die &ldquo;Personensuche&rdquo;: <strong>Blonde Haare, dein Lächeln, schwarze Jacke</strong>.</p>
<h2 id="methodik-und-fehlerquellen">Methodik und Fehlerquellen</h2>
<h3 id="datenerfassung-und-grobes-datenmodell">Datenerfassung und grobes Datenmodell</h3>
<p>Um die Beiträge <a href="https://www.bvg.de/de/Meine-BVG/Meine-Augenblicke/Alle-Augenblicke">von der Hauptseite</a> abzugreifen, nutze ich ein in <strong>PHP</strong> geschriebenes Script (<a href="https://github.com/nickyreinert/crawl-augenblicke">crawl-augenblicke auf Github</a>). In einer <strong>MySQL</strong>-Tabelle speichere ich dann den <strong>Titel der Nachricht</strong>, den <strong>Nachrichten-Text</strong>, den <strong>Verfasser der Nachricht</strong>, das <strong>Datum</strong> an dem die Nachricht verfasst wurde sowie das <strong>Datum, an dem der &ldquo;Augenblick&rdquo; stattgefunden</strong> hat. Dazu wird die <strong>URL</strong> zum Beitrag sowie die <strong>Linie</strong> erfasst. Zusätzlich nutze ich einige selbstgeschriebene <strong>MySQL-Funktionen</strong>, um die <strong>Anzahl der Wörter im Titel</strong> und dem <strong>Nachrichten-Text</strong> sowie die <strong>Differenz zwischen den beiden Zeitpunkten</strong> zu erfassen. Ein paar eigene Views erleichtern das anfängliche Finden von Fehlern.</p>
<p>Daneben gibt es zwei weitere Tabellen mit Meta-Informationen. Dies ist zunächst eine <strong>Blacklist</strong>-Tabelle, die bei einer Aggregation der Wort-Häufigkeiten ignoriert werden sollen. Das betrifft nicht den Vorgang des Zählens der Wörter! In einer weiteren Tabelle wird eine <strong>Zuordnung der Linien</strong> zu den Linientypen sowie Subtypen festgehalten. Zu den Linientypen zählen nur Bus, Tram, Zug sowie Fähre. Anhand der Subtypen kann außerdem in besondere Liniengruppen unterschieden werden, wie z.B. Metro-Bus, Express-Bus usw.</p>
<p><a href="https://www.nickyreinert.de/files/augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/datenbankschema.png"><img src="/2021/2021-01-21-augenblicke-eine-statistische-analyse-des-flirt-portals-der-bvg/images/datenbankschema-300x182.png" alt="Datenbankschema"></a></p>
<p>Datenbankschema</p>
<p>Mit einem <strong>Python-Script</strong> lese ich den Datensatz ein und bereinige die Texte. Stoppwörter werden aussortiert, Steuerzeichen, HTML-Entitites und andere Störquellen werden entfernt. Dann berechen ich für jedes Wort die <strong>IDF</strong> (<strong>Inverse Document Frequency</strong>) nach der Formel <strong>log 1 + (doc_count_total / doc count with word occurrence</strong>). Für die NLP-Aufgaben nutze ich das NLTK bzw. den <strong>HannoverTagger</strong> (<a href="https://nickyreinert.de/blog/2020/12/09/einfuehrung-in-stemming-und-lemmatisierung-deutscher-texte-mit-python/">siehe auch</a>). Für die Erkennung der Phrasen und der Stimmung nutze ich <strong>TextBlob</strong> mit deutschen <a href="https://github.com/markuskiller/textblob-de">Trainingsdaten von hier</a>.</p>
<h3 id="mögliche-fehlerquellen">Mögliche Fehlerquellen</h3>
<p>Grundsätzlich gibt es zwei Dinge zu beobachten: Die Auswertungen werden nicht in Relation zur tatsächlichen Anzahl der Fahrgäste der jeweiligen Linie betrachtet, da diese nur auf Jahresebene zu Verfügung stehen. Es kann also nur ein Vergleich der absoluten Zahlen stattfinden. In Relation betrachtet kann die Gegenüberstellung der Linien ein anderes Ergebnis liefern.</p>
<p>Außerdem kann die Anzahl der Einträge je Linie auf zwei Arten interpretiert werden: Die Passagiere sind in der Linie mit vielen Einträgen entsprechend flirtfreudiger und offener und deshalb kommt es auf der Plattform zu mehr Gesuchen. Oder aber es ist genau andersrum: Da die Menschen einer bestimmten Linie schüchterner sind, trauen sie sich erst im Nachhinein die Kontaktaufnahme über diese Plattform zu starten. Diese Erkenntnisse müssen also mit Vorsicht betrachtet werden.</p>
<p>Leider gab es im Laufe der Zeit einige technische Anpassungen auf den Seiten der BVG, die einige Analysen etwas erschweren bzw. verhindern. Das betrifft vor allem das Datum, an dem die Beiträge verfasst (<strong>date_posted</strong>) wurden. Es fällt auf, dass sehr viele Einträge offenbar am 13. August 2014 verfasst wurden, das Datum des Augenblicks (<strong>date_met</strong>) aber sehr lange zurück liegt, teilweise bis 2006. Das älteste Datum in date_posted ist der 30.06.2014, bei date_met allerdings der 14.02.2006. Die Vermutung ist also, dass das Datum, an dem der Beitrag verfasst wurde, erst ab Juni 2014 mit erfasst wurde. Im August hat man dann vermutlich alle älteren Beiträge auf das feste Datum, nämlich den 13.08.2014 gesetzt. Insgesamt betrifft das immerhin 15.560 Datensätze.</p>
<p>Die Stimmungsanalyse nutzt einen fertigen Trainingsdatensatz und ist nur so gut, wie die Qualität der Texte.</p>
<h2 id="verwendete-technologie">Verwendete Technologie</h2>
<p>Für das Abgreifen des BVG-Portals verwende ich ein <a href="https://github.com/nickyreinert/crawl-augenblicke">PHP-Script</a>. Das Python-Script zur Normalisierung, Anreicherung und Bereinigung der Daten ist nicht öffentlich. Ganz offensichtlich verwende ich Wordpress. In einer früheren Variante habe ich zwei selbstgeschriebene Plugins verwendet. Das ist zum einen das Plugin für die Darstellung der <a href="https://github.com/nickyreinert/data-heatmap">HTML-Heatmap</a> sowie das Plugin für die <a href="https://github.com/nickyreinert/wordCloud-for-Wordpress">WordCloud</a>, basierend auf dem fantastischen wordCloud2.js von Tim Dream. Die Idee war, die Zahlen dynamisch zu aktualisieren. Mittlerweile bin ich auf einfache Screenshots von Tableau Public umgestiegen.</p>

        
        
        <div class="tags">
          <p><strong>Tags:</strong> analyse, augenblicke, berlin, bvg, oepnv, statistik, tinder, Berlin</p>
        </div>
        
      ]]></content:encoded>
      
      
      
      <category>projekte</category>
      
      
      
      
      <media:content url="https://nickyreinert.de/images/posts/placeholder.jpg" type="image/jpeg">
        <media:title>Augenblicke - Eine statistische Analyse des Flirt-Portals der BVG - Titelbild</media:title>
      </media:content>
      
      
      
      
      <dc:subject>Lesezeit: 5 Minuten</dc:subject>
      
      
      
      <dc:type>data_analysis</dc:type>
      
      
    </item><item>
      <title>Einführung in Stemming und Lemmatisierung deutscher Texte mit Python</title>
      <link>https://nickyreinert.de/2020/2020-12-09-einfuehrung-in-stemming-und-lemmatisierung-deutscher-texte-mit-python/</link>
      <pubDate>Wed, 09 Dec 2020 00:00:00 +0000</pubDate>
      <author></author>
      <guid>https://nickyreinert.de/2020/2020-12-09-einfuehrung-in-stemming-und-lemmatisierung-deutscher-texte-mit-python/</guid>
      <description>Um beim Text Mining zusammengehörende Wörter zu gruppieren, bedient man sich im Natural Language Processing (NLP) zweier Methoden: Lemmatisierung (lemmatising) …</description>
      
      
      <content:encoded>&lt;![CDATA[
        
        <div class="ai-summary">
          <h3>AI-Zusammenfassung</h3>
          <p>Eine technische Einführung und ein Vergleich verschiedener Python-Bibliotheken für das Stemming und die Lemmatisierung deutscher Texte. Der Artikel erklärt die Konzepte, stellt Bibliotheken wie NLTK, SpaCy, HannoverTagger und IWNLP vor, zeigt deren Installation und Anwendung anhand von Code-Beispielen und vergleicht ihre Ergebnisse bei der Normalisierung deutscher Wörter.</p>
          
          
          <p><strong>Hauptthemen:</strong> Python, Natural Language Processing, NLP, Text Mining, Stemming, Lemmatisierung, Data Science</p>
          
          
          
          <p><strong>Schwierigkeitsgrad:</strong> advanced</p>
          
        </div>
        
        
        <p>Um beim <strong>Text Mining</strong> zusammengehörende Wörter zu gruppieren, bedient man sich im <strong>Natural Language Processing</strong> (<strong>NLP</strong>) zweier Methoden: <strong>Lemmatisierung</strong> (lemmatising) und <strong>Stemming</strong>. Das ist notwendig, um z.B. einen Text besser kategorisieren bzw. mit Stichworten versehen zu können. Eines der wichtigsten Anwendungsgebiete ist sicherlich die Indexierung von Dokumenten durch eine Suchmaschine. Ein ganz einfaches Beispiel: Enthält ein Dokument sehr oft das Wort <em>Häuser</em> und der Nutzer sucht nach dem Begriff <em>Haus</em>, wird das relevante Dokument wohl nicht in den Suchergebnissen auftauchen.</p>
<p>Um das zu umgehen, müssen flektierte und abgeleitete Wörter zu Ihrer Grundform zurückgeführt werden. Beim <strong>Stemming</strong> werden dazu einfache heuritische Methoden angewendet, bei dem das Suffix der Wörter entfernt wird. Aus dem Wort <em>Katzen</em> wird so dessen Grundform <em>Katze</em>. Bei der Plural-Form <em>Häuser</em> ist das etwas schwieriger. Mit dem Abschneiden des Suffixes kommt man hier nicht weit, weshalb man sich bei der <strong>Lemmatisierung</strong> an Listen bzw. Datenbanken orientiert, die die reflektierte Formen enthalten und so eine sichere Verknüpfung von <em>Häuser</em> zur Singular-Form <em>Haus</em> erlauben.</p>
<p>Soviel zur Theorie. In der Praxis gibt es <strong>Python</strong> und eine Vielzahl von Modulen, die einem eine Menge Arbeit abnehmen. Im Folgenden vergleiche ich ein halbes Dutzend Module, die die <strong>Lemmatisierung</strong> und das <strong>Stemming</strong> beherrschen und auch für <strong>Deutsche Texte</strong> anwendbar sind.</p>
<p><em>Hinweis: Zur Vorbereitung beim Text Mining gehören natürlich auch das Säubern des Textes, Entfernen von Stop-Wörtern und das <strong>Tokenizing</strong>, also Aufbrechen eines Satzes in seine einzelnen Bestandteile. Diesen Schritt überspringe ich hier.</em></p>
<h2 id="stemming-mit-porter-lancaster-und-snowball">Stemming mit Porter, Lancaster und Snowball</h2>
<p>Für das Stemming habe ich mir drei Module angeschaut:</p>
<ul>
<li>Porter Stemmer</li>
<li>Lancaster Stemmer</li>
<li>Snowball Stemmer</li>
</ul>
<p>Der <strong>Porter Stemmer</strong> wurde bereits 1979 von <strong>Martin F. Porter</strong> entwickelt und <a href="https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html">gilt zumindest in der englischen Sprache als sehr effektiv</a>. Der <strong>Lancaster Stammer</strong> geht auf den Ende der 1980er Jahre an der Lancaster University von <strong>Chris Paice</strong> und <strong>Gareth Husk</strong> entwickelten Paice-Husk Stemming Algorithmus zurück. Im Gegensatz zum festen Regelsatz von Porter wird <a href="https://www.scientificpsychic.com/paice/paice.html">beim Lancaster mit externen Regeln gearbeitet</a>, womit der Algorithmus flexibler ist.</p>
<p>Der Snowball Stemmer ist eigentlich kein eigener Algorithmus, <a href="https://www.datacamp.com/community/tutorials/stemming-lemmatization-python">sondern eine Sprache</a>, um einen eigenen Stemmer zu schreiben.</p>
<h3 id="installation-und-anwendung">Installation und Anwendung</h3>
<p>Alle drei Module sind Bestandteil des <a href="https://www.nltk.org/">Natural Language Toolkit</a> und können dementsprechend sehr unkompliziert mit <strong>pip install nltk</strong> installiert werden. Danach sieht ein Anwendungsbeispiel folgendermaßen aus:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="ln"> 1</span><span class="cl"> from nltk.stem import PorterStemmer
</span></span><span class="line"><span class="ln"> 2</span><span class="cl"> from nltk.stem import LancasterStemmer
</span></span><span class="line"><span class="ln"> 3</span><span class="cl"> from nltk.stem.snowball import SnowballStemmer
</span></span><span class="line"><span class="ln"> 4</span><span class="cl"> 
</span></span><span class="line"><span class="ln"> 5</span><span class="cl"> porter = PorterStemmer()
</span></span><span class="line"><span class="ln"> 6</span><span class="cl"> lancaster = LancasterStemmer()
</span></span><span class="line"><span class="ln"> 7</span><span class="cl"> snowball = SnowballStemmer(&#34;german&#34;)
</span></span><span class="line"><span class="ln"> 8</span><span class="cl"> 
</span></span><span class="line"><span class="ln"> 9</span><span class="cl"> word = &#39;Katzen&#39;
</span></span><span class="line"><span class="ln">10</span><span class="cl"> 
</span></span><span class="line"><span class="ln">11</span><span class="cl"> print (&#39;Porter: &#39; + porter.stem(word))
</span></span><span class="line"><span class="ln">12</span><span class="cl"> print (&#39;Lancaster: &#39; + lancaster.stem(word))
</span></span><span class="line"><span class="ln">13</span><span class="cl"> print (&#39;Snowball: &#39; + snowball.stem(word))
</span></span></code></pre></div><p>Da Snowball mehrere Sprachen unterstützt, muss hier vorher festgelegt werden, mit welcher Sprache wir arbeiten. Der Rest ist eigentlich ziemlich straight forward. Das Ergebnis zeigt aber die Schwächen des Stemmings:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="ln">1</span><span class="cl">Porter: katzen 
</span></span><span class="line"><span class="ln">2</span><span class="cl">Lancaster: katz 
</span></span><span class="line"><span class="ln">3</span><span class="cl">Snowball: katz 
</span></span></code></pre></div><p>Keiner der Stemmer kommt auf <em>Katze</em>. Klar: Hier werden einfach nur ein paar Buchstaben abgeschnitten. Da Porter nicht für die deutsche Sprache ausgelegt ist, wird hier sogar die reflektierte Form zurückgegeben. Das Stemming kann also dabei helfen, reflektierte Wörter auf einen gemeinsamen Stamm zu reduzieren. Die Bedeutung geht dabei aber oft verloren.</p>
<p>Genau deshalb gibt es die <strong>Lemmatisierung</strong>&hellip;</p>
<h2 id="lemmatisieren-mit-hannovertagger-wordnet-spacy-und-iwnlp">Lemmatisieren mit HannoverTagger, WordNet, Spacy und IWNLP</h2>
<p>Für die Lemmatisierung habe ich vier Module herausgesucht. Vor allem <a href="https://www.machinelearningplus.com/nlp/lemmatization-examples-python/">für die englische Sprache ist die Auswahl aber weitaus größer</a>, für deutsche Texte ist es hingegen schwierig, aktuelle und gepflegte Module zu finden.</p>
<ul>
<li>WordNet</li>
<li>SpaCy</li>
<li>HannoverTagger</li>
<li>IWNLP</li>
</ul>
<p>Das <strong>WordNet</strong> Modul gehört ebenfalls zum NLTK und ist einer der am weitesten verbreiteten Lemmatiser. Das Modul wurde 2001 entwickelt; <strong>WordNet</strong> selber ist eine riesige lexikalische Datenbank, die seit 1985 an der <strong>Princeton University</strong> entwickelt wird und mittlerweile über 200 Sprachen unterstützt.</p>
<p><strong>SpaCy</strong> ist vergleichsweise jung (2015) aber mittlerweile auch sehr weit verbreitet. Im Gegensatz zum NLTK, dass eine Vielzahl von Lösungen und Algorithmen mitbringt, konzentriert sich SpaCy auf genau einen Algorithmus, um ein Problem zu lösen und ist damit ein wenig fokussierter als das NLTK. Während das NLTK eher im Forschungsbereich anzutreffen ist, wird SpaCy vornehmlich im produktiven Bereich verwendet.</p>
<p>Der <strong>HannoverTagger</strong>, kurz <strong>HanTa</strong> - <a href="https://www.rki.de/DE/Content/Infekt/EpidBull/Merkblaetter/Ratgeber_Hantaviren.html">nicht zu verwechseln mit dem gleichnamigen Virus</a>, wurde 2019 <a href="https://textmining.wp.hs-hannover.de/Preprocessing.html">an der Hochschule Hannover</a> entwickelt. HanTa wurde mit dem Ziel entwickelt, auch für deutsche Texte eine vernünftige Lemmatisierungs-Lösung zu besitzen.</p>
<p>Daneben gibt es noch <a href="https://github.com/Liebeck/spacy-iwnlp">IWNLP</a> von <strong>Matthias Liebeck</strong>. IWNLP ist der Name der entsprechenden SpaCy-Erweiterung für <a href="https://github.com/Liebeck/iwnlp-py">IWNLP-py</a>, was wiederum die Python-Implementierung von IWNLP ist: <strong>Inverse Wiktionary for Natural Language Processing</strong>. IWNLP nutzt zur Lemmatisierung einfach den Deutschen Bereich des Wiktionaries.</p>
<h3 id="was-ist-mit-germalemma-und-german-lemmatizer">Was ist mit GermaLemma und German Lemmatizer?</h3>
<p><strong><a href="https://github.com/WZBSocialScienceCenter/germalemma">GermaLemma</a></strong> ist ein weiteres, recht junges Modul von <strong>Markus Konrad</strong>, das aber leider die <strong>POS</strong> der Wörter erfordert. POS steht für <strong>Part-Of-Speech</strong>, also die Wortart, wie z.B. Substantiv, Verb, Adjektiv und so weiter. Da alle anderen Lemmatizer ohne die POS arbeiten und ich die einfachste Lösung gesucht habe, bleibt dieses Modul außen vor.</p>
<p>Eine weitere Lösung wäre <a href="https://pypi.org/project/german-lemmatizer/">Docker-Image</a> mit dem Namen <strong>German Lemmatizer</strong> gewesen, dass die Funktionen von <strong>INWLP</strong> und <strong>GermaLemma</strong> kombiniert. Das ganze lässt sich aber leider nur mit etwas Mehraufwand auch außerhalb von Docker nutzen, weshalb ich auch den <strong>German Lemmatizer</strong> hier nicht berücksichtigt habe.</p>
<p><strong>WordNet</strong> kann Wörter übrigens ohne POS lemmatisieren, die Ergebnisse sind mit POS aber weitaus genauer. Die Klassifizierung des POS ist freilich keine Raketenwissenschaft und <a href="https://www.machinelearningplus.com/nlp/lemmatization-examples-python/">z.B. hier recht gut beschrieben</a>.</p>
<h3 id="installation-und-anwendung-1">Installation und Anwendung</h3>
<p>Da wir oben schon das NLTK installiert haben, können wir direkt auf WordNet zugreifen. SpaCy installieren wir mit <strong>pip install spacy</strong>, dort wird dann auch gleich IWNLP mitgeliefert. Der HanTa lässt sich ebenfalls unkompliziert installieren: <strong>pip install HanTa</strong>.</p>
<p>Um IWNLP zum Laufen zu bringen, benötigen wir noch <a href="http://lager.cs.uni-duesseldorf.de/NLP/IWNLP/">den letzten Dump von hier</a> (letzter Stand 2018/10/01). Das Archiv enthälte eine JSON-Datei - das Lexikon mit etwa drölfizigtausend Lemmas. Um SpaCy für die deutsche Sprache anwendbar zumachen, <a href="https://spacy.io/models/de/">müssen wir hier ein komplettes Modell herunterladen</a>. Das übernimmt SpaCy für uns mit folgendem Befehl:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-gdscript3" data-lang="gdscript3"><span class="line"><span class="ln">1</span><span class="cl"><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spacy</span> <span class="n">download</span> <span class="n">de_core_news_md</span>
</span></span></code></pre></div><p><em>(Es gibt drei verschieden große Modelle, ich habe mich für die goldene Mitte entschieden).</em></p>
<p>Die Implementierung ist dann etwas aufwendiger, da bei der Lemmatisierung Trainingsmodelle eingesetzt werden, und nicht nur &ldquo;einfache&rdquo; Algorithmen:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-gdscript3" data-lang="gdscript3"><span class="line"><span class="ln"> 1</span><span class="cl"> <span class="n">from</span> <span class="n">HanTa</span> <span class="n">import</span> <span class="n">HanoverTagger</span> <span class="n">as</span> <span class="n">ht</span>
</span></span><span class="line"><span class="ln"> 2</span><span class="cl"> <span class="n">from</span> <span class="n">nltk</span><span class="o">.</span><span class="n">stem</span> <span class="n">import</span> <span class="n">WordNetLemmatizer</span>
</span></span><span class="line"><span class="ln"> 3</span><span class="cl"> <span class="n">from</span> <span class="n">spacy_iwnlp</span> <span class="n">import</span> <span class="n">spaCyIWNLP</span>
</span></span><span class="line"><span class="ln"> 4</span><span class="cl"> <span class="n">import</span> <span class="n">spacy</span>
</span></span><span class="line"><span class="ln"> 5</span><span class="cl"> 
</span></span><span class="line"><span class="ln"> 6</span><span class="cl"> <span class="n">hannover</span> <span class="o">=</span> <span class="n">ht</span><span class="o">.</span><span class="n">HanoverTagger</span><span class="p">(</span><span class="s1">&#39;morphmodel_ger.pgz&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="ln"> 7</span><span class="cl"> <span class="n">wordnet</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>
</span></span><span class="line"><span class="ln"> 8</span><span class="cl"> 
</span></span><span class="line"><span class="ln"> 9</span><span class="cl"> <span class="n">spc</span> <span class="o">=</span>  <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;/usr/local/lib/python3.9/site-packages/de_core_news_md/de_core_news_md-2.3.0&#39;</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;parser&#39;</span><span class="p">,</span> <span class="s1">&#39;ner&#39;</span><span class="p">])</span>
</span></span><span class="line"><span class="ln">10</span><span class="cl"> 
</span></span><span class="line"><span class="ln">11</span><span class="cl"> <span class="n">iwnlp</span> <span class="o">=</span> <span class="n">spc</span>
</span></span><span class="line"><span class="ln">12</span><span class="cl"> <span class="n">iwnlp_pipe</span> <span class="o">=</span> <span class="n">spaCyIWNLP</span><span class="p">(</span><span class="n">lemmatizer_path</span><span class="o">=</span><span class="s1">&#39;/Users/user1/Downloads/IWNLP.Lemmatizer_20181001.json&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">13</span><span class="cl"> <span class="n">iwnlp</span><span class="o">.</span><span class="n">add_pipe</span><span class="p">(</span><span class="n">iwnlp_pipe</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">14</span><span class="cl"> 
</span></span><span class="line"><span class="ln">15</span><span class="cl"> <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;HanTa:&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">hannover</span><span class="o">.</span><span class="n">analyze</span><span class="p">(</span><span class="n">word</span><span class="p">)[</span><span class="mi">0</span><span class="p">]))</span>
</span></span><span class="line"><span class="ln">16</span><span class="cl"> <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Wordnet:&#39;</span> <span class="o">+</span> <span class="n">wordnet</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="s1">&#39;NN&#39;</span><span class="p">))</span>
</span></span><span class="line"><span class="ln">17</span><span class="cl"> <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;SpaCy:&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">([</span><span class="n">token</span><span class="o">.</span><span class="n">lemma_</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">spc</span><span class="p">(</span><span class="n">word</span><span class="p">)][</span><span class="mi">0</span><span class="p">]))</span>
</span></span><span class="line"><span class="ln">18</span><span class="cl"> <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;IWNLP:&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">([</span><span class="n">token</span><span class="o">.</span><span class="n">lemma_</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">iwnlp</span><span class="p">(</span><span class="n">word</span><span class="p">)][</span><span class="mi">0</span><span class="p">]))</span>
</span></span></code></pre></div><p>Dem HannoverTagger wird bei der Initialisierung das entsprechende Modell mitgegeben. WordNet benötigt keine weiteren Parameter. Der <strong>SpaCy-Lemmatiser</strong> (hier spc) wird mit dem deutschen News Paket über <em>spacy.load()</em> initialisiert. Alternativ funktioniert die Initialisierung auch über den Shortcut <em>de</em>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-gdscript3" data-lang="gdscript3"><span class="line"><span class="ln">1</span><span class="cl"> <span class="n">spc</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;de&#39;</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;parser&#39;</span><span class="p">,</span> <span class="s1">&#39;ner&#39;</span><span class="p">])</span>
</span></span></code></pre></div><p>Um den Vorgang etwas zu beschleunigen, deaktivieren wir die Funktionen <em>parser</em> und <em>ner</em>. Der Parser macht bei der Verarbeitung von Sätzen Sinn, <em>ner</em> steht für <strong>Name Entity Recognition</strong>, sprich die Erkennung von Eigennamen. Das ist bei der Lemmatisierung durchaus wichtig, möchte ich hier aber erstmal nicht berücksichtigen.</p>
<p>Da <strong>IWNLP</strong> ebenfalls über SpaCy aktiviert wird, klonen wir das Objekt einfach und fügen dann unsere eigene Pipeline hinzu <em>add_pipe()</em>. Diese muss auf das Lexikon als JSON-Datei verweisen, den wir vorher heruntergeladen haben. Das wars auch schon, danach sehen wir, wie sich die Module untereinander und im Vergleich zum Stemming schlagen:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="ln">1</span><span class="cl">HanTa:Katze 
</span></span><span class="line"><span class="ln">2</span><span class="cl">Wordnet:Katzen
</span></span><span class="line"><span class="ln">3</span><span class="cl">SpaCy:Katze
</span></span><span class="line"><span class="ln">4</span><span class="cl">IWNLP:Katze 
</span></span></code></pre></div><p>Das Ergebnis überrascht nur ein kleines bisschen: Obwohl die eigentliche WordNet-Datenbank 200 Sprachen unterstützt, schafft es das Modul nicht, das richtige Lemma zuzuordnen. Ich finde leider auch keine Informationen dazu, wie WordNet auf Deutsch getrimmt werden kann. Die Ergebnisse von HanTa, SpaCy und IWNLP passen allerdings. IWNLP (und demnach auch SpaCy) liefern bei Bedarf übrigens mehr als nur ein Lemma zurück:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="ln">1</span><span class="cl"> print (str([token._.iwnlp_lemmas for token in iwnlp(word)][0]))
</span></span><span class="line"><span class="ln">2</span><span class="cl"> print (str([token.pos_ for token in iwnlp(word)][0]))
</span></span></code></pre></div><p>Mit <strong>pos_</strong> und <strong>_.iwnlp_lemmas</strong> bekommt man einerseits den Part-Of-Speech und in IWNLP sogar eine Liste aller denkbaren Lemmas - sofern zutreffend.</p>
<p>Wer den Vergleich etwas hübscher aufbereitet für mehrer Wörter nutzen will, <a href="https://gist.github.com/nickyreinert/72548ce88d812f9203687ece93c608d8">kann dazu folgendes Jupyter-Notebook nutzen</a>. Ich hab das ganze mal <a href="https://www.spiegel.de/wissenschaft/mensch/uno-bericht-ueber-klimaziele-auf-dem-weg-zu-drei-grad-erderwaermung-a-05ecdcdb-2f84-4aa0-84b6-a7a2d8c71c80">für eine beliebige Schlagzeile von Spiegel Online</a> durchgespielt und folgendes Ergebnis erhalten. Der Original-Satz lautet:</p>
<blockquote>
<p>Der weltweite CO₂-Ausstoß steigt weiter – trotz Corona-Knick, heißt es im neuen Bericht des Uno-Umweltprogramms. Ohne grüne Konjunkturpakete sei das Pariser Zwei-Grad-Limit nicht mehr zu retten.</p></blockquote>
<p>Und das ist das Ergebnis nach dem <strong>Steming</strong> und <strong>Lemmatisieren</strong>:</p>
<p><a href="https://www.nickyreinert.de/files/eine-kleine-einfuhrung-in-das-stemming-lemmatisieren-deutscher-texte/grafik.png"><img src="/2020/2020-12-09-einfuehrung-in-stemming-und-lemmatisierung-deutscher-texte-mit-python/images/grafik-700x249.png" alt=""></a></p>
<p>7 Methoden zum Stemming und Lemmatisieren im Vergleich</p>
<p>Das Stemming liefert, naturgemäß, ein relativ grobes Ergebnis ab. Um die Wörter in einem Text schnell zu kategorisieren reicht das sicherlich aus. Bei der Lemmatisierung fällt auf, das SpaCy trotz deutscher Sprachpakete nicht ganz zufriedenstellend arbeitet. So wird aus &ldquo;das&rdquo; z.B. &ldquo;der&rdquo;. Dafür wird &ldquo;trotz&rdquo; korrekt &ldquo;trotzen&rdquo; zugeordenet - was dem HanTa nicht gelingt. HanTa wiederum kennt als einziger den Singular von Konjunkturpakete.</p>
<h2 id="fazit">Fazit</h2>
<p>Die Verarbeitung englischer Texte ist, aufgrund der großen Verbreitung der Sprache, gar kein Problem. Bei deutschen Texten wird es schon etwas schwieriger, allerdings liefern <strong>HanTa</strong>, <strong>IWNLP</strong> und auch <strong>SpaCy</strong> recht gute Ergebnisse ab. Mein subjektiver Favorit ist <strong>HanTa</strong>. Aber die Stichprobe ist viel zu gering, um hier einen klaren Favoriten identifizieren zu können.</p>
<p>Der Vergleich dient eher nicht als repräsentivate Untersuchung aller denkbaren Varianten, soll aber einen kleinen Einblick in <strong>NLP</strong> und die automatisierte Text-Verarbeitung im <strong>Text-Mining</strong> geben und ein paar Code-Beispiele liefern, um den Einsteig in Python zu erleichtern. Ich hoffe das ist gelungen!</p>

        
        
        <div class="tags">
          <p><strong>Tags:</strong> Python, NLP, Tutorial, Data-Science, Text-Mining</p>
        </div>
        
      ]]></content:encoded>
      
      
      
      <category>blog</category>
      
      
      
      
      <media:content url="https://nickyreinert.de/images/posts/placeholder.jpg" type="image/jpeg">
        <media:title>Einführung in Stemming und Lemmatisierung deutscher Texte mit Python - Titelbild</media:title>
      </media:content>
      
      
      
      
      <dc:subject>Lesezeit: 5 Minuten</dc:subject>
      
      
      
      <dc:type>technical_comparison</dc:type>
      
      
    </item>
  </channel>
</rss>