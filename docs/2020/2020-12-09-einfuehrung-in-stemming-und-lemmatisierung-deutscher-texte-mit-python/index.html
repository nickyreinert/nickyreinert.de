<!DOCTYPE html>
<html lang="de-DE">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">

  
  <meta name="author" content="">

  
  
  <meta name="description" content="Um beim Text Mining zusammengehörende Wörter zu gruppieren, bedient man sich im Natural Language Processing (NLP) zweier Methoden: Lemmatisierung (lemmatising) und Stemming. Das ist notwendig, um z.B. einen Text besser kategorisieren bzw. mit Stichworten versehen zu können. Eines der wichtigsten Anwendungsgebiete ist sicherlich die Indexierung von Dokumenten durch eine Suchmaschine. Ein ganz einfaches Beispiel: Enthält ein Dokument sehr oft das Wort Häuser und der Nutzer sucht nach dem Begriff Haus, wird das relevante Dokument wohl nicht in den Suchergebnissen auftauchen.">
  

  
  <link rel="icon" href="https://nickyreinert.de/images/favicon.svg">

  
  
  <meta name="keywords" content=" Python  NLP  Stemming  Lemmatisierung  NLTK  SpaCy  HannoverTagger  IWNLP  Text Mining  German NLP  Data Science  Textverarbeitung ">
  

  
  
  
<link rel="stylesheet" href="/css/katex.min.css">
<script defer src="/js/katex.min.js"></script>
<script defer src="/js/auto-render.min.js"></script>
<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: '$$', right: '$$', display: true },
        { left: '\\[', right: '\\]', display: true },
        { left: '$', right: '$', display: false },
        { left: '\\(', right: '\\)', display: false }
      ],
      ignoredTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code', 'option'],
      throwOnError: false
    });
  });
</script>


  

  
  <meta property="og:url" content="https://nickyreinert.de/2020/2020-12-09-einfuehrung-in-stemming-und-lemmatisierung-deutscher-texte-mit-python/">
  <meta property="og:site_name" content="Nicky Reinert">
  <meta property="og:title" content="Einführung in Stemming und Lemmatisierung deutscher Texte mit Python">
  <meta property="og:description" content="Um beim Text Mining zusammengehörende Wörter zu gruppieren, bedient man sich im Natural Language Processing (NLP) zweier Methoden: Lemmatisierung (lemmatising) und Stemming. Das ist notwendig, um z.B. einen Text besser kategorisieren bzw. mit Stichworten versehen zu können. Eines der wichtigsten Anwendungsgebiete ist sicherlich die Indexierung von Dokumenten durch eine Suchmaschine. Ein ganz einfaches Beispiel: Enthält ein Dokument sehr oft das Wort Häuser und der Nutzer sucht nach dem Begriff Haus, wird das relevante Dokument wohl nicht in den Suchergebnissen auftauchen.">
  <meta property="og:locale" content="de_DE">
  <meta property="og:type" content="article">
    <meta property="article:section" content="2020">
    <meta property="article:published_time" content="2020-12-09T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-10-11T11:01:31+02:00">
    <meta property="article:tag" content="Python">
    <meta property="article:tag" content="NLP">
    <meta property="article:tag" content="Tutorial">
    <meta property="article:tag" content="Data-Science">
    <meta property="article:tag" content="Text-Mining">


  
  <link rel="canonical" href="https://nickyreinert.de/2020/2020-12-09-einfuehrung-in-stemming-und-lemmatisierung-deutscher-texte-mit-python/">

  
  
  
  <meta itemprop="name" content="Einführung in Stemming und Lemmatisierung deutscher Texte mit Python">
  <meta itemprop="description" content="Um beim Text Mining zusammengehörende Wörter zu gruppieren, bedient man sich im Natural Language Processing (NLP) zweier Methoden: Lemmatisierung (lemmatising) und Stemming. Das ist notwendig, um z.B. einen Text besser kategorisieren bzw. mit Stichworten versehen zu können. Eines der wichtigsten Anwendungsgebiete ist sicherlich die Indexierung von Dokumenten durch eine Suchmaschine. Ein ganz einfaches Beispiel: Enthält ein Dokument sehr oft das Wort Häuser und der Nutzer sucht nach dem Begriff Haus, wird das relevante Dokument wohl nicht in den Suchergebnissen auftauchen.">
  <meta itemprop="datePublished" content="2020-12-09T00:00:00+00:00">
  <meta itemprop="dateModified" content="2025-10-11T11:01:31+02:00">
  <meta itemprop="wordCount" content="1564">
  <meta itemprop="keywords" content="Python,NLP,Stemming,Lemmatisierung,NLTK,SpaCy,HannoverTagger,IWNLP,Text Mining,German NLP,Data Science,Textverarbeitung">

  
  <link media="screen" rel="stylesheet" href='https://nickyreinert.de/css/common.css'>
  <link media="screen" rel="stylesheet" href='https://nickyreinert.de/css/content.css'>

  
  
  <title>Einführung in Stemming und Lemmatisierung deutscher Texte mit Python - Nicky Reinert</title>
  

  
  
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Einführung in Stemming und Lemmatisierung deutscher Texte mit Python">
  <meta name="twitter:description" content="Um beim Text Mining zusammengehörende Wörter zu gruppieren, bedient man sich im Natural Language Processing (NLP) zweier Methoden: Lemmatisierung (lemmatising) und Stemming. Das ist notwendig, um z.B. einen Text besser kategorisieren bzw. mit Stichworten versehen zu können. Eines der wichtigsten Anwendungsgebiete ist sicherlich die Indexierung von Dokumenten durch eine Suchmaschine. Ein ganz einfaches Beispiel: Enthält ein Dokument sehr oft das Wort Häuser und der Nutzer sucht nach dem Begriff Haus, wird das relevante Dokument wohl nicht in den Suchergebnissen auftauchen.">


  
<script type="application/ld+json">
[
  {
    "@context": "https://schema.org",
    "@type": "BlogPosting",
    "headline": "Einführung in Stemming und Lemmatisierung deutscher Texte mit Python",
    "description": "Um beim Text Mining zusammengehörende Wörter zu gruppieren, bedient man sich im Natural Language Processing (NLP) zweier Methoden: Lemmatisierung (lemmatising) …",
    "datePublished": "2020-12-09T00:00:00Z",
    "dateModified": "2025-10-11T11:01:31\u002b02:00",
    "author": {
      "@type": "Person",
      "name": "",
      "url": "https:\/\/nickyreinert.de\/"
    },
    "publisher": {
      "@type": "Organization",
      "name": "Nicky Reinert",
      "url": "https:\/\/nickyreinert.de\/",
      "logo": {
        "@type": "ImageObject",
        "url": "https:\/\/nickyreinert.de\/images\/logo.png"
      }
    },
    "mainEntityOfPage": {
      "@type": "WebPage",
      "@id": "https:\/\/nickyreinert.de\/2020\/2020-12-09-einfuehrung-in-stemming-und-lemmatisierung-deutscher-texte-mit-python\/"
    },
    
    "image": {
      "@type": "ImageObject",
      "url": "https:\/\/nickyreinert.de\/images\/posts\/placeholder.jpg",
      "width": 1200,
      "height": 630
    },
    
    "url": "https:\/\/nickyreinert.de\/2020\/2020-12-09-einfuehrung-in-stemming-und-lemmatisierung-deutscher-texte-mit-python\/",
    "wordCount":  1564 ,
    
    "timeRequired": "PT5M",
    
    
    "keywords": ["Python", "NLP", "Tutorial", "Data-Science", "Text-Mining"],
    
    
    "articleSection": ["anleitungen"],
    
    
    "abstract": "Eine technische Einführung und ein Vergleich verschiedener Python-Bibliotheken für das Stemming und die Lemmatisierung deutscher Texte. Der Artikel erklärt die Konzepte, stellt Bibliotheken wie NLTK, SpaCy, HannoverTagger und IWNLP vor, zeigt deren Installation und Anwendung anhand von Code-Beispielen und vergleicht ihre Ergebnisse bei der Normalisierung deutscher Wörter.",
    
    
    "genre": "technical_comparison",
    
    
    "educationalLevel": "advanced",
    
    "inLanguage": "de"
  },
  
  {
    "@context": "https://schema.org",
    "@type": "TechArticle",
    "headline": "Einführung in Stemming und Lemmatisierung deutscher Texte mit Python",
    "description": "Um beim Text Mining zusammengehörende Wörter zu gruppieren, bedient man sich im Natural Language Processing (NLP) zweier Methoden: Lemmatisierung (lemmatising) …",
    
    "proficiencyLevel": "advanced",
    
    
    "dependencies": ["Gute Python-Kenntnisse", "Erfahrung mit der Installation von Python-Paketen (pip)", "Grundlegendes Verständnis von NLP-Konzepten"],
    
    
    "about": [
      
      
      {
        "@type": "Thing",
        "name": "Python"
      }
      
      , 
      {
        "@type": "Thing",
        "name": "Natural Language Processing"
      }
      
      , 
      {
        "@type": "Thing",
        "name": "NLP"
      }
      
      , 
      {
        "@type": "Thing",
        "name": "Text Mining"
      }
      
      , 
      {
        "@type": "Thing",
        "name": "Stemming"
      }
      
      , 
      {
        "@type": "Thing",
        "name": "Lemmatisierung"
      }
      
      , 
      {
        "@type": "Thing",
        "name": "Data Science"
      }
      
    ],
    
    "datePublished": "2020-12-09T00:00:00Z",
    "dateModified": "2025-10-11T11:01:31\u002b02:00",
    "author": {
      "@type": "Person",
      "name": ""
    }
  },
  
  {
    "@context": "https://schema.org",
    "@type": "WebSite",
    "name": "Nicky Reinert",
    "url": "https:\/\/nickyreinert.de\/",
    "description": "Blog \u0026 Projekte von Nicky Reinert (Institut für digitale Herausforderungen): Webentwicklung \u0026 Software Development, KI, SEO \u0026 Analytics, Hosting \u0026 DevOps, WordPress \u0026 Hugo, Tools \u0026 Projekte, Datenschutz und digitale Kultur",
    "inLanguage": "de",
    "potentialAction": {
      "@type": "SearchAction",
      "target": {
        "@type": "EntryPoint",
        "urlTemplate": "https:\/\/nickyreinert.de\/search/?q={search_term_string}"
      },
      "query-input": "required name=search_term_string"
    }
  }
]
</script>

  
<link rel="stylesheet" href='https://nickyreinert.de/css/single.css'>

  

<script defer src="/js/alpine.min.js"></script>



  
  

<link rel="webmention" href="https://webmention.io/nickyreinert.de/webmention" />



</head>

<body>
  <div id="wrapper">
    <header id="header">
  <span class="header-content">
    <p class="left"><a id="site-title" href="https://nickyreinert.de/">Nicky Reinert</a></p>
    <p class="center">//</p>
    <p class="right">Institut für digitale Herausforderungen</p>
  </span>

  <br />
  
  <div class="footnote">
    <span class="nav-bar-item"><b>extern</b>
      <a target="_blank" class="link" href="https://medium.com/@nickyreinert">medium.com</a>
      <a target="_blank" class="link" href="https://www.linkedin.com/in/nickyreinert/">LinkedIn</a>
      <a target="_blank" class="link" href="https://www.instagram.com/nickyreinert/?hl=de">Instagram</a>
      <a target="_blank" class="link" href="https://www.youtube.com/channel/UC832c48LxmzLd5nIL_Ny7sA">YouTube</a>
      <a target="_blank" class="link" href="https://github.com/nickyreinert">GitHub</a>
      <a target="_blank" class="link" href="https://not-a-brand.myspreadshop.de/">"Merch"</a>
      <a target="_blank" class="link" href="https://ministerium-für-digitale-herausforderungen.de">ministerium</a>
      <a target="_blank" class="link" href="https://institut-fdh.de//">institut</a>
      <a target="_blank" class="link" href="https://fantastokrat.de/">fantastokrat</a>
      <a target="_blank" class="link" href="https://open.spotify.com/show/3383SHVRGTvJ9cEXU3UuE5">random knowledge</a>
    </span>

    <br />

    <span class="nav-bar-item"><b>literatur</b>
      <a target="_blank" class="link" href="https://amzn.eu/d/5LYXyUu">Dit is Berlin - B.N.H.K.</a>
      <a target="_blank" class="link" href="https://amzn.eu/d/3hWCrK1">Geometrische Gottheiten</a>
    </span>
    
    <nav><b>intern</b>
      
      <span class="nav-bar-item">
        <a class="link" href="/categories/blog/">blog</a>
      </span>
      
      <span class="nav-bar-item">
        <a class="link" href="/categories/analysen/">analysen</a>
      </span>
      
      <span class="nav-bar-item">
        <a class="link" href="/categories/nerdenz/">nerdenz</a>
      </span>
      
      <span class="nav-bar-item">
        <a class="link" href="/categories/anleitungen/">anleitungen</a>
      </span>
      
      <span class="nav-bar-item">
        <a class="link" href="/categories/wordpress/">wordpress</a>
      </span>
      
      <span class="nav-bar-item">
        <a class="link" href="/categories/hosting/">hosting</a>
      </span>
      
      <span class="nav-bar-item">
        <a class="link" href="/categories/tools/">tools</a>
      </span>
      
      <span class="nav-bar-item">
        <a class="link" href="/categories/development/">development</a>
      </span>
      
      <span class="nav-bar-item">
        <a class="link" href="/categories/projekte/">projekte</a>
      </span>
      
      <span class="nav-bar-item">
        <a class="link" href="/categories/autismus/">autismus</a>
      </span>
      
    </nav>
    
  </div>

  <br />

</header>



    
<main id="main" class="post">
  
  
  <div class="post-header">
    <div class="navigation" style="align-items: center;">
      <div style="flex-shrink: 0; width: 50px;">
        
        <a class="link prev" href="/2020/2020-11-05-wie-funktionieren-dateirechte/" style="font-size: 1.2rem;">&lt;&lt;&lt;</a>
        
      </div>
      
      <h1 style="flex: 1; text-align: center; min-width: 0; padding: 0 0.5rem;" itemprop="headline">Einführung in Stemming und Lemmatisierung deutscher Texte mit Python</h1>
      
      <div style="flex-shrink: 0; width: 50px; text-align: right;">
        
      </div>
    </div>
  </div>
  
  
  <div class="post-meta" style="font-size: 0.9rem; margin-top: 0.5rem; text-align: center; color: #878787;">
    December 9, 2020 &nbsp;|&nbsp; Kategorie: 
    
    <a class="link" href='https://nickyreinert.de/categories/anleitungen' style="text-decoration: none;">anleitungen</a>
    
  </div>
    
  
  
  
  <details style="margin-top: 0.5rem; margin-bottom: 2rem; font-size: 0.9rem; color: #878787; background: none; border: none;">
    <summary>
      <b>Table of Contents</b>
    </summary>
    <div class="toc" style="margin-left: 1rem;">
      <nav id="TableOfContents">
  <ul>
    <li><a href="#stemming-mit-porter-lancaster-und-snowball">Stemming mit Porter, Lancaster und Snowball</a>
      <ul>
        <li><a href="#installation-und-anwendung">Installation und Anwendung</a></li>
      </ul>
    </li>
    <li><a href="#lemmatisieren-mit-hannovertagger-wordnet-spacy-und-iwnlp">Lemmatisieren mit HannoverTagger, WordNet, Spacy und IWNLP</a>
      <ul>
        <li><a href="#was-ist-mit-germalemma-und-german-lemmatizer">Was ist mit GermaLemma und German Lemmatizer?</a></li>
        <li><a href="#installation-und-anwendung-1">Installation und Anwendung</a></li>
      </ul>
    </li>
    <li><a href="#fazit">Fazit</a></li>
  </ul>
</nav>
    </div>
  </details>
  
  
  
  
  <article style="border-top: 2px solid #878787;" class="content">
    
    <p>Um beim <strong>Text Mining</strong> zusammengehörende Wörter zu gruppieren, bedient man sich im <strong>Natural Language Processing</strong> (<strong>NLP</strong>) zweier Methoden: <strong>Lemmatisierung</strong> (lemmatising) und <strong>Stemming</strong>. Das ist notwendig, um z.B. einen Text besser kategorisieren bzw. mit Stichworten versehen zu können. Eines der wichtigsten Anwendungsgebiete ist sicherlich die Indexierung von Dokumenten durch eine Suchmaschine. Ein ganz einfaches Beispiel: Enthält ein Dokument sehr oft das Wort <em>Häuser</em> und der Nutzer sucht nach dem Begriff <em>Haus</em>, wird das relevante Dokument wohl nicht in den Suchergebnissen auftauchen.</p>
<p>Um das zu umgehen, müssen flektierte und abgeleitete Wörter zu Ihrer Grundform zurückgeführt werden. Beim <strong>Stemming</strong> werden dazu einfache heuritische Methoden angewendet, bei dem das Suffix der Wörter entfernt wird. Aus dem Wort <em>Katzen</em> wird so dessen Grundform <em>Katze</em>. Bei der Plural-Form <em>Häuser</em> ist das etwas schwieriger. Mit dem Abschneiden des Suffixes kommt man hier nicht weit, weshalb man sich bei der <strong>Lemmatisierung</strong> an Listen bzw. Datenbanken orientiert, die die reflektierte Formen enthalten und so eine sichere Verknüpfung von <em>Häuser</em> zur Singular-Form <em>Haus</em> erlauben.</p>
<p>Soviel zur Theorie. In der Praxis gibt es <strong>Python</strong> und eine Vielzahl von Modulen, die einem eine Menge Arbeit abnehmen. Im Folgenden vergleiche ich ein halbes Dutzend Module, die die <strong>Lemmatisierung</strong> und das <strong>Stemming</strong> beherrschen und auch für <strong>Deutsche Texte</strong> anwendbar sind.</p>
<p><em>Hinweis: Zur Vorbereitung beim Text Mining gehören natürlich auch das Säubern des Textes, Entfernen von Stop-Wörtern und das <strong>Tokenizing</strong>, also Aufbrechen eines Satzes in seine einzelnen Bestandteile. Diesen Schritt überspringe ich hier.</em></p>
<h2 id="stemming-mit-porter-lancaster-und-snowball">Stemming mit Porter, Lancaster und Snowball</h2>
<p>Für das Stemming habe ich mir drei Module angeschaut:</p>
<ul>
<li>Porter Stemmer</li>
<li>Lancaster Stemmer</li>
<li>Snowball Stemmer</li>
</ul>
<p>Der <strong>Porter Stemmer</strong> wurde bereits 1979 von <strong>Martin F. Porter</strong> entwickelt und 





  
  

  
  

  
  
  

  
    
      
    
  

  
  <a href="https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html?utm_source=institut-fuer-digitale-herausforderungen&amp;utm_medium=allyourbasearebelongtous" target="_blank" rel="noopener noreferrer">gilt zumindest in der englischen Sprache als sehr effektiv</a>


. Der <strong>Lancaster Stammer</strong> geht auf den Ende der 1980er Jahre an der Lancaster University von <strong>Chris Paice</strong> und <strong>Gareth Husk</strong> entwickelten Paice-Husk Stemming Algorithmus zurück. Im Gegensatz zum festen Regelsatz von Porter wird 





  
  

  
  

  
  
  

  
    
      
    
  

  
  <a href="https://www.scientificpsychic.com/paice/paice.html?utm_source=institut-fuer-digitale-herausforderungen&amp;utm_medium=allyourbasearebelongtous" target="_blank" rel="noopener noreferrer">beim Lancaster mit externen Regeln gearbeitet</a>


, womit der Algorithmus flexibler ist.</p>
<p>Der Snowball Stemmer ist eigentlich kein eigener Algorithmus, 





  
  

  
  

  
  
  

  
    
      
    
  

  
  <a href="https://www.datacamp.com/community/tutorials/stemming-lemmatization-python?utm_source=institut-fuer-digitale-herausforderungen&amp;utm_medium=allyourbasearebelongtous" target="_blank" rel="noopener noreferrer">sondern eine Sprache</a>


, um einen eigenen Stemmer zu schreiben.</p>
<h3 id="installation-und-anwendung">Installation und Anwendung</h3>
<p>Alle drei Module sind Bestandteil des 





  
  

  
  

  
  
  

  
    
      
    
  

  
  <a href="https://www.nltk.org/?utm_source=institut-fuer-digitale-herausforderungen&amp;utm_medium=allyourbasearebelongtous" target="_blank" rel="noopener noreferrer">Natural Language Toolkit</a>


 und können dementsprechend sehr unkompliziert mit <strong>pip install nltk</strong> installiert werden. Danach sieht ein Anwendungsbeispiel folgendermaßen aus:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="ln"> 1</span><span class="cl"> from nltk.stem import PorterStemmer
</span></span><span class="line"><span class="ln"> 2</span><span class="cl"> from nltk.stem import LancasterStemmer
</span></span><span class="line"><span class="ln"> 3</span><span class="cl"> from nltk.stem.snowball import SnowballStemmer
</span></span><span class="line"><span class="ln"> 4</span><span class="cl"> 
</span></span><span class="line"><span class="ln"> 5</span><span class="cl"> porter = PorterStemmer()
</span></span><span class="line"><span class="ln"> 6</span><span class="cl"> lancaster = LancasterStemmer()
</span></span><span class="line"><span class="ln"> 7</span><span class="cl"> snowball = SnowballStemmer(&#34;german&#34;)
</span></span><span class="line"><span class="ln"> 8</span><span class="cl"> 
</span></span><span class="line"><span class="ln"> 9</span><span class="cl"> word = &#39;Katzen&#39;
</span></span><span class="line"><span class="ln">10</span><span class="cl"> 
</span></span><span class="line"><span class="ln">11</span><span class="cl"> print (&#39;Porter: &#39; + porter.stem(word))
</span></span><span class="line"><span class="ln">12</span><span class="cl"> print (&#39;Lancaster: &#39; + lancaster.stem(word))
</span></span><span class="line"><span class="ln">13</span><span class="cl"> print (&#39;Snowball: &#39; + snowball.stem(word))
</span></span></code></pre></div><p>Da Snowball mehrere Sprachen unterstützt, muss hier vorher festgelegt werden, mit welcher Sprache wir arbeiten. Der Rest ist eigentlich ziemlich straight forward. Das Ergebnis zeigt aber die Schwächen des Stemmings:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="ln">1</span><span class="cl">Porter: katzen 
</span></span><span class="line"><span class="ln">2</span><span class="cl">Lancaster: katz 
</span></span><span class="line"><span class="ln">3</span><span class="cl">Snowball: katz 
</span></span></code></pre></div><p>Keiner der Stemmer kommt auf <em>Katze</em>. Klar: Hier werden einfach nur ein paar Buchstaben abgeschnitten. Da Porter nicht für die deutsche Sprache ausgelegt ist, wird hier sogar die reflektierte Form zurückgegeben. Das Stemming kann also dabei helfen, reflektierte Wörter auf einen gemeinsamen Stamm zu reduzieren. Die Bedeutung geht dabei aber oft verloren.</p>
<p>Genau deshalb gibt es die <strong>Lemmatisierung</strong>&hellip;</p>
<h2 id="lemmatisieren-mit-hannovertagger-wordnet-spacy-und-iwnlp">Lemmatisieren mit HannoverTagger, WordNet, Spacy und IWNLP</h2>
<p>Für die Lemmatisierung habe ich vier Module herausgesucht. Vor allem 





  
  

  
  

  
  
  

  
    
      
    
  

  
  <a href="https://www.machinelearningplus.com/nlp/lemmatization-examples-python/?utm_source=institut-fuer-digitale-herausforderungen&amp;utm_medium=allyourbasearebelongtous" target="_blank" rel="noopener noreferrer">für die englische Sprache ist die Auswahl aber weitaus größer</a>


, für deutsche Texte ist es hingegen schwierig, aktuelle und gepflegte Module zu finden.</p>
<ul>
<li>WordNet</li>
<li>SpaCy</li>
<li>HannoverTagger</li>
<li>IWNLP</li>
</ul>
<p>Das <strong>WordNet</strong> Modul gehört ebenfalls zum NLTK und ist einer der am weitesten verbreiteten Lemmatiser. Das Modul wurde 2001 entwickelt; <strong>WordNet</strong> selber ist eine riesige lexikalische Datenbank, die seit 1985 an der <strong>Princeton University</strong> entwickelt wird und mittlerweile über 200 Sprachen unterstützt.</p>
<p><strong>SpaCy</strong> ist vergleichsweise jung (2015) aber mittlerweile auch sehr weit verbreitet. Im Gegensatz zum NLTK, dass eine Vielzahl von Lösungen und Algorithmen mitbringt, konzentriert sich SpaCy auf genau einen Algorithmus, um ein Problem zu lösen und ist damit ein wenig fokussierter als das NLTK. Während das NLTK eher im Forschungsbereich anzutreffen ist, wird SpaCy vornehmlich im produktiven Bereich verwendet.</p>
<p>Der <strong>HannoverTagger</strong>, kurz <strong>HanTa</strong> - 





  
  

  
  

  
  
  

  
    
      
    
  

  
  <a href="https://www.rki.de/DE/Content/Infekt/EpidBull/Merkblaetter/Ratgeber_Hantaviren.html?utm_source=institut-fuer-digitale-herausforderungen&amp;utm_medium=allyourbasearebelongtous" target="_blank" rel="noopener noreferrer">nicht zu verwechseln mit dem gleichnamigen Virus</a>


, wurde 2019 





  
  

  
  

  
  
  

  
    
      
    
  

  
  <a href="https://textmining.wp.hs-hannover.de/Preprocessing.html?utm_source=institut-fuer-digitale-herausforderungen&amp;utm_medium=allyourbasearebelongtous" target="_blank" rel="noopener noreferrer">an der Hochschule Hannover</a>


 entwickelt. HanTa wurde mit dem Ziel entwickelt, auch für deutsche Texte eine vernünftige Lemmatisierungs-Lösung zu besitzen.</p>
<p>Daneben gibt es noch 





  
  

  
  

  
  
  

  
    
      
    
  

  
  <a href="https://github.com/Liebeck/spacy-iwnlp?utm_source=institut-fuer-digitale-herausforderungen&amp;utm_medium=allyourbasearebelongtous" target="_blank" rel="noopener noreferrer">IWNLP</a>


 von <strong>Matthias Liebeck</strong>. IWNLP ist der Name der entsprechenden SpaCy-Erweiterung für 





  
  

  
  

  
  
  

  
    
      
    
  

  
  <a href="https://github.com/Liebeck/iwnlp-py?utm_source=institut-fuer-digitale-herausforderungen&amp;utm_medium=allyourbasearebelongtous" target="_blank" rel="noopener noreferrer">IWNLP-py</a>


, was wiederum die Python-Implementierung von IWNLP ist: <strong>Inverse Wiktionary for Natural Language Processing</strong>. IWNLP nutzt zur Lemmatisierung einfach den Deutschen Bereich des Wiktionaries.</p>
<h3 id="was-ist-mit-germalemma-und-german-lemmatizer">Was ist mit GermaLemma und German Lemmatizer?</h3>
<p><strong>





  
  

  
  

  
  
  

  
    
      
    
  

  
  <a href="https://github.com/WZBSocialScienceCenter/germalemma?utm_source=institut-fuer-digitale-herausforderungen&amp;utm_medium=allyourbasearebelongtous" target="_blank" rel="noopener noreferrer">GermaLemma</a>


</strong> ist ein weiteres, recht junges Modul von <strong>Markus Konrad</strong>, das aber leider die <strong>POS</strong> der Wörter erfordert. POS steht für <strong>Part-Of-Speech</strong>, also die Wortart, wie z.B. Substantiv, Verb, Adjektiv und so weiter. Da alle anderen Lemmatizer ohne die POS arbeiten und ich die einfachste Lösung gesucht habe, bleibt dieses Modul außen vor.</p>
<p>Eine weitere Lösung wäre 





  
  

  
  

  
  
  

  
    
      
    
  

  
  <a href="https://pypi.org/project/german-lemmatizer/?utm_source=institut-fuer-digitale-herausforderungen&amp;utm_medium=allyourbasearebelongtous" target="_blank" rel="noopener noreferrer">Docker-Image</a>


 mit dem Namen <strong>German Lemmatizer</strong> gewesen, dass die Funktionen von <strong>INWLP</strong> und <strong>GermaLemma</strong> kombiniert. Das ganze lässt sich aber leider nur mit etwas Mehraufwand auch außerhalb von Docker nutzen, weshalb ich auch den <strong>German Lemmatizer</strong> hier nicht berücksichtigt habe.</p>
<p><strong>WordNet</strong> kann Wörter übrigens ohne POS lemmatisieren, die Ergebnisse sind mit POS aber weitaus genauer. Die Klassifizierung des POS ist freilich keine Raketenwissenschaft und 





  
  

  
  

  
  
  

  
    
      
    
  

  
  <a href="https://www.machinelearningplus.com/nlp/lemmatization-examples-python/?utm_source=institut-fuer-digitale-herausforderungen&amp;utm_medium=allyourbasearebelongtous" target="_blank" rel="noopener noreferrer">z.B. hier recht gut beschrieben</a>


.</p>
<h3 id="installation-und-anwendung-1">Installation und Anwendung</h3>
<p>Da wir oben schon das NLTK installiert haben, können wir direkt auf WordNet zugreifen. SpaCy installieren wir mit <strong>pip install spacy</strong>, dort wird dann auch gleich IWNLP mitgeliefert. Der HanTa lässt sich ebenfalls unkompliziert installieren: <strong>pip install HanTa</strong>.</p>
<p>Um IWNLP zum Laufen zu bringen, benötigen wir noch 





  
  

  
  

  
  
  

  
    
      
    
  

  
  <a href="http://lager.cs.uni-duesseldorf.de/NLP/IWNLP/?utm_source=institut-fuer-digitale-herausforderungen&amp;utm_medium=allyourbasearebelongtous" target="_blank" rel="noopener noreferrer">den letzten Dump von hier</a>


 (letzter Stand 2018/10/01). Das Archiv enthälte eine JSON-Datei - das Lexikon mit etwa drölfizigtausend Lemmas. Um SpaCy für die deutsche Sprache anwendbar zumachen, 





  
  

  
  

  
  
  

  
    
      
    
  

  
  <a href="https://spacy.io/models/de/?utm_source=institut-fuer-digitale-herausforderungen&amp;utm_medium=allyourbasearebelongtous" target="_blank" rel="noopener noreferrer">müssen wir hier ein komplettes Modell herunterladen</a>


. Das übernimmt SpaCy für uns mit folgendem Befehl:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-gdscript3" data-lang="gdscript3"><span class="line"><span class="ln">1</span><span class="cl"><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">spacy</span> <span class="n">download</span> <span class="n">de_core_news_md</span>
</span></span></code></pre></div><p><em>(Es gibt drei verschieden große Modelle, ich habe mich für die goldene Mitte entschieden).</em></p>
<p>Die Implementierung ist dann etwas aufwendiger, da bei der Lemmatisierung Trainingsmodelle eingesetzt werden, und nicht nur &ldquo;einfache&rdquo; Algorithmen:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-gdscript3" data-lang="gdscript3"><span class="line"><span class="ln"> 1</span><span class="cl"> <span class="n">from</span> <span class="n">HanTa</span> <span class="n">import</span> <span class="n">HanoverTagger</span> <span class="n">as</span> <span class="n">ht</span>
</span></span><span class="line"><span class="ln"> 2</span><span class="cl"> <span class="n">from</span> <span class="n">nltk</span><span class="o">.</span><span class="n">stem</span> <span class="n">import</span> <span class="n">WordNetLemmatizer</span>
</span></span><span class="line"><span class="ln"> 3</span><span class="cl"> <span class="n">from</span> <span class="n">spacy_iwnlp</span> <span class="n">import</span> <span class="n">spaCyIWNLP</span>
</span></span><span class="line"><span class="ln"> 4</span><span class="cl"> <span class="n">import</span> <span class="n">spacy</span>
</span></span><span class="line"><span class="ln"> 5</span><span class="cl"> 
</span></span><span class="line"><span class="ln"> 6</span><span class="cl"> <span class="n">hannover</span> <span class="o">=</span> <span class="n">ht</span><span class="o">.</span><span class="n">HanoverTagger</span><span class="p">(</span><span class="s1">&#39;morphmodel_ger.pgz&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="ln"> 7</span><span class="cl"> <span class="n">wordnet</span> <span class="o">=</span> <span class="n">WordNetLemmatizer</span><span class="p">()</span>
</span></span><span class="line"><span class="ln"> 8</span><span class="cl"> 
</span></span><span class="line"><span class="ln"> 9</span><span class="cl"> <span class="n">spc</span> <span class="o">=</span>  <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;/usr/local/lib/python3.9/site-packages/de_core_news_md/de_core_news_md-2.3.0&#39;</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;parser&#39;</span><span class="p">,</span> <span class="s1">&#39;ner&#39;</span><span class="p">])</span>
</span></span><span class="line"><span class="ln">10</span><span class="cl"> 
</span></span><span class="line"><span class="ln">11</span><span class="cl"> <span class="n">iwnlp</span> <span class="o">=</span> <span class="n">spc</span>
</span></span><span class="line"><span class="ln">12</span><span class="cl"> <span class="n">iwnlp_pipe</span> <span class="o">=</span> <span class="n">spaCyIWNLP</span><span class="p">(</span><span class="n">lemmatizer_path</span><span class="o">=</span><span class="s1">&#39;/Users/user1/Downloads/IWNLP.Lemmatizer_20181001.json&#39;</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">13</span><span class="cl"> <span class="n">iwnlp</span><span class="o">.</span><span class="n">add_pipe</span><span class="p">(</span><span class="n">iwnlp_pipe</span><span class="p">)</span>
</span></span><span class="line"><span class="ln">14</span><span class="cl"> 
</span></span><span class="line"><span class="ln">15</span><span class="cl"> <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;HanTa:&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">hannover</span><span class="o">.</span><span class="n">analyze</span><span class="p">(</span><span class="n">word</span><span class="p">)[</span><span class="mi">0</span><span class="p">]))</span>
</span></span><span class="line"><span class="ln">16</span><span class="cl"> <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;Wordnet:&#39;</span> <span class="o">+</span> <span class="n">wordnet</span><span class="o">.</span><span class="n">lemmatize</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="s1">&#39;NN&#39;</span><span class="p">))</span>
</span></span><span class="line"><span class="ln">17</span><span class="cl"> <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;SpaCy:&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">([</span><span class="n">token</span><span class="o">.</span><span class="n">lemma_</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">spc</span><span class="p">(</span><span class="n">word</span><span class="p">)][</span><span class="mi">0</span><span class="p">]))</span>
</span></span><span class="line"><span class="ln">18</span><span class="cl"> <span class="nb">print</span> <span class="p">(</span><span class="s1">&#39;IWNLP:&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">([</span><span class="n">token</span><span class="o">.</span><span class="n">lemma_</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">iwnlp</span><span class="p">(</span><span class="n">word</span><span class="p">)][</span><span class="mi">0</span><span class="p">]))</span>
</span></span></code></pre></div><p>Dem HannoverTagger wird bei der Initialisierung das entsprechende Modell mitgegeben. WordNet benötigt keine weiteren Parameter. Der <strong>SpaCy-Lemmatiser</strong> (hier spc) wird mit dem deutschen News Paket über <em>spacy.load()</em> initialisiert. Alternativ funktioniert die Initialisierung auch über den Shortcut <em>de</em>:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-gdscript3" data-lang="gdscript3"><span class="line"><span class="ln">1</span><span class="cl"> <span class="n">spc</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;de&#39;</span><span class="p">,</span> <span class="n">disable</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;parser&#39;</span><span class="p">,</span> <span class="s1">&#39;ner&#39;</span><span class="p">])</span>
</span></span></code></pre></div><p>Um den Vorgang etwas zu beschleunigen, deaktivieren wir die Funktionen <em>parser</em> und <em>ner</em>. Der Parser macht bei der Verarbeitung von Sätzen Sinn, <em>ner</em> steht für <strong>Name Entity Recognition</strong>, sprich die Erkennung von Eigennamen. Das ist bei der Lemmatisierung durchaus wichtig, möchte ich hier aber erstmal nicht berücksichtigen.</p>
<p>Da <strong>IWNLP</strong> ebenfalls über SpaCy aktiviert wird, klonen wir das Objekt einfach und fügen dann unsere eigene Pipeline hinzu <em>add_pipe()</em>. Diese muss auf das Lexikon als JSON-Datei verweisen, den wir vorher heruntergeladen haben. Das wars auch schon, danach sehen wir, wie sich die Module untereinander und im Vergleich zum Stemming schlagen:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="ln">1</span><span class="cl">HanTa:Katze 
</span></span><span class="line"><span class="ln">2</span><span class="cl">Wordnet:Katzen
</span></span><span class="line"><span class="ln">3</span><span class="cl">SpaCy:Katze
</span></span><span class="line"><span class="ln">4</span><span class="cl">IWNLP:Katze 
</span></span></code></pre></div><p>Das Ergebnis überrascht nur ein kleines bisschen: Obwohl die eigentliche WordNet-Datenbank 200 Sprachen unterstützt, schafft es das Modul nicht, das richtige Lemma zuzuordnen. Ich finde leider auch keine Informationen dazu, wie WordNet auf Deutsch getrimmt werden kann. Die Ergebnisse von HanTa, SpaCy und IWNLP passen allerdings. IWNLP (und demnach auch SpaCy) liefern bei Bedarf übrigens mehr als nur ein Lemma zurück:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="ln">1</span><span class="cl"> print (str([token._.iwnlp_lemmas for token in iwnlp(word)][0]))
</span></span><span class="line"><span class="ln">2</span><span class="cl"> print (str([token.pos_ for token in iwnlp(word)][0]))
</span></span></code></pre></div><p>Mit <strong>pos_</strong> und <strong>_.iwnlp_lemmas</strong> bekommt man einerseits den Part-Of-Speech und in IWNLP sogar eine Liste aller denkbaren Lemmas - sofern zutreffend.</p>
<p>Wer den Vergleich etwas hübscher aufbereitet für mehrer Wörter nutzen will, 





  
  

  
  

  
  
  

  
    
      
    
  

  
  <a href="https://gist.github.com/nickyreinert/72548ce88d812f9203687ece93c608d8?utm_source=institut-fuer-digitale-herausforderungen&amp;utm_medium=allyourbasearebelongtous" target="_blank" rel="noopener noreferrer">kann dazu folgendes Jupyter-Notebook nutzen</a>


. Ich hab das ganze mal 





  
  

  
  

  
  
  

  
    
      
    
  

  
  <a href="https://www.spiegel.de/wissenschaft/mensch/uno-bericht-ueber-klimaziele-auf-dem-weg-zu-drei-grad-erderwaermung-a-05ecdcdb-2f84-4aa0-84b6-a7a2d8c71c80?utm_source=institut-fuer-digitale-herausforderungen&amp;utm_medium=allyourbasearebelongtous" target="_blank" rel="noopener noreferrer">für eine beliebige Schlagzeile von Spiegel Online</a>


 durchgespielt und folgendes Ergebnis erhalten. Der Original-Satz lautet:</p>
<blockquote>
<p>Der weltweite CO₂-Ausstoß steigt weiter – trotz Corona-Knick, heißt es im neuen Bericht des Uno-Umweltprogramms. Ohne grüne Konjunkturpakete sei das Pariser Zwei-Grad-Limit nicht mehr zu retten.</p></blockquote>
<p>Und das ist das Ergebnis nach dem <strong>Steming</strong> und <strong>Lemmatisieren</strong>:</p>
<p>





  
  

  
  

  
  
  

  
    
      
    
  

  
  <a href="https://www.nickyreinert.de/files/eine-kleine-einfuhrung-in-das-stemming-lemmatisieren-deutscher-texte/grafik.png?utm_source=institut-fuer-digitale-herausforderungen&amp;utm_medium=allyourbasearebelongtous" target="_blank" rel="noopener noreferrer"><img src="/2020/2020-12-09-einfuehrung-in-stemming-und-lemmatisierung-deutscher-texte-mit-python/images/grafik-700x249.png" alt=""></a>


</p>
<p>7 Methoden zum Stemming und Lemmatisieren im Vergleich</p>
<p>Das Stemming liefert, naturgemäß, ein relativ grobes Ergebnis ab. Um die Wörter in einem Text schnell zu kategorisieren reicht das sicherlich aus. Bei der Lemmatisierung fällt auf, das SpaCy trotz deutscher Sprachpakete nicht ganz zufriedenstellend arbeitet. So wird aus &ldquo;das&rdquo; z.B. &ldquo;der&rdquo;. Dafür wird &ldquo;trotz&rdquo; korrekt &ldquo;trotzen&rdquo; zugeordenet - was dem HanTa nicht gelingt. HanTa wiederum kennt als einziger den Singular von Konjunkturpakete.</p>
<h2 id="fazit">Fazit</h2>
<p>Die Verarbeitung englischer Texte ist, aufgrund der großen Verbreitung der Sprache, gar kein Problem. Bei deutschen Texten wird es schon etwas schwieriger, allerdings liefern <strong>HanTa</strong>, <strong>IWNLP</strong> und auch <strong>SpaCy</strong> recht gute Ergebnisse ab. Mein subjektiver Favorit ist <strong>HanTa</strong>. Aber die Stichprobe ist viel zu gering, um hier einen klaren Favoriten identifizieren zu können.</p>
<p>Der Vergleich dient eher nicht als repräsentivate Untersuchung aller denkbaren Varianten, soll aber einen kleinen Einblick in <strong>NLP</strong> und die automatisierte Text-Verarbeitung im <strong>Text-Mining</strong> geben und ein paar Code-Beispiele liefern, um den Einsteig in Python zu erleichtern. Ich hoffe das ist gelungen!</p>

    
  </article>
  
  
  
  <div style="margin-top: 5rem; border-top: 2px solid #9a9a9a; padding: 1rem; color: #878787;">
    <h2 style="font-size: 1.3rem; margin-bottom: 1rem;">Zusammenfassung</h2>
    <div style="font-size: 1rem;">

      <p>Eine technische Einführung und ein Vergleich verschiedener Python-Bibliotheken für das Stemming und die Lemmatisierung deutscher Texte. Der Artikel erklärt die Konzepte, stellt Bibliotheken wie NLTK, SpaCy, HannoverTagger und IWNLP vor, zeigt deren Installation und Anwendung anhand von Code-Beispielen und vergleicht ihre Ergebnisse bei der Normalisierung deutscher Wörter.</p>
      <br />
      
      <p><strong>Hauptthemen:</strong>
        
        <span style="background: #e0e0e0; padding: 0.2rem 0.5rem; margin: 0.1rem; border-radius: 3px;">Python</span>
        
        <span style="background: #e0e0e0; padding: 0.2rem 0.5rem; margin: 0.1rem; border-radius: 3px;">Natural Language Processing</span>
        
        <span style="background: #e0e0e0; padding: 0.2rem 0.5rem; margin: 0.1rem; border-radius: 3px;">NLP</span>
        
        <span style="background: #e0e0e0; padding: 0.2rem 0.5rem; margin: 0.1rem; border-radius: 3px;">Text Mining</span>
        
        <span style="background: #e0e0e0; padding: 0.2rem 0.5rem; margin: 0.1rem; border-radius: 3px;">Stemming</span>
        
        <span style="background: #e0e0e0; padding: 0.2rem 0.5rem; margin: 0.1rem; border-radius: 3px;">Lemmatisierung</span>
        
        <span style="background: #e0e0e0; padding: 0.2rem 0.5rem; margin: 0.1rem; border-radius: 3px;">Data Science</span>
        
      </p>
      
      
      
      <p><strong>Schwierigkeitsgrad:</strong> advanced</p>
      
      
      
      <p><strong>Lesezeit:</strong> ca. 5 Minuten</p>
      
    </div>
  </div>
  
  
  
  <div class="paginator">
    
    <a class="link" href="https://nickyreinert.de/2020/2020-11-05-wie-funktionieren-dateirechte/">← prev</a>
    
    
    <a></a>
    
  </div>
  
</main>

    <footer id="footer">
  <div>
    <span>© 1979 Nicky Reinert</span> // <span>Powered by </span>
    <a class="link" href="https://gohugo.io/">Hugo</a> // 
    <span>kontakt: nickyreinert -at- gmail -dot- com</span> // 
    <a class="link" href="privacy.html">Datenschutzerklärung</a> // 
    <bold>
      <a class="link lang-btn" href="#" data-lang="de" >
                    <svg width="16" height="12" viewBox="0 0 5 3" xmlns="http://www.w3.org/2000/svg" style="display: inline-block; vertical-align: middle; margin-right: 8px; flex-shrink: 0;">
                    <rect width="5" height="3" fill="#FFCE00"/>
                    <rect width="5" height="2" fill="#DD0000"/>
                    <rect width="5" height="1" fill="#000000"/>
                  </svg>Deutsch</a> // 
      <a class="link lang-btn" href="#" data-lang="en" >
                          <svg width="16" height="12" viewBox="0 0 60 30" xmlns="http://www.w3.org/2000/svg" style="display: inline-block; vertical-align: middle; margin-right: 8px; flex-shrink: 0;">
                    <clipPath id="s-v2">
                      <path d="M0,0 v30 h60 v-30 z"/>
                    </clipPath>
                    <clipPath id="s-v1">
                      <path d="M30,15 h30 v15 z v15 h-30 z h-30 v-15 z v-15 h30 z"/>
                    </clipPath>
                    <g clip-path="url(#s-v2)">
                      <path d="M0,0 v30 h60 v-30 z" fill="#00247d"/>
                      <g clip-path="url(#s-v1)">
                        <path d="M0,0 l60,30 M60,0 l-60,30" stroke="#fff" stroke-width="6"/>
                        <path d="M0,0 l60,30 M60,0 l-60,30" stroke="#cf142b" stroke-width="4"/>
                        <path d="M30,0 v30 M0,15 h60" stroke="#fff" stroke-width="10"/>
                        <path d="M30,0 v30 M0,15 h60" stroke="#cf142b" stroke-width="6"/>
                      </g>
                    </g>
                  </svg>English</a> // 
      <a class="link lang-btn" href="#" data-lang="*" >All(e)</a>
    </bold>
    <br />
<footer>
  <center>
    <a class="default-link" href="https://uberblogr.de/prev/nickyreinert" title="Zurück im Ring">&lt; Zurück im Ring</a> //
    Mitglied im <a class="default-link" href="https://uberblogr.de/home/nickyreinert" title="Mitglied im UberBlogr Webring">UberBlogr Webring</a> //
    <a class="default-link" href="https://uberblogr.de/next/nickyreinert" title="Vor im Ring">Weiter im Ring &gt;</a>
  </center>
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      var langButtons = document.querySelectorAll('.lang-btn');
      langButtons.forEach(function(btn) {
        btn.addEventListener('click', function(event) {
          event.preventDefault();
          var lang = btn.getAttribute('data-lang');
          var posts = document.querySelectorAll('[data-lang]');
            posts.forEach(function(post) {
              var postLang = post.getAttribute('data-lang');
              if (postLang === lang || lang === '*') {
                post.classList.remove('hidden');
              } else {
                post.classList.add('hidden');
              }
            });
          });
        });
      });
  </script>
</footer>
  </div>

  
  

  
  

</body>

</html>
