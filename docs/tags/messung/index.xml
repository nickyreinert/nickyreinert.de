<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" 
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:media="http://search.yahoo.com/mrss/">
  <channel>
    <title>Messung auf Nicky Reinert</title>
    <link>https://nickyreinert.de/tags/messung/</link>
    <description>Blog &amp; Projekte von Nicky Reinert (Institut für digitale Herausforderungen): Webentwicklung &amp; Software Development, SEO &amp; Analytics, Hosting &amp; DevOps, WordPress &amp; Hugo, Tools &amp; Projekte, Datenschutz und digitale Kultur – plus Texte zu KI sowie Autismus &amp; Gesellschaft.</description>
    <generator>Hugo 0.148.2</generator>
    <language>de</language>
    <managingEditor></managingEditor>
    <webMaster></webMaster>
    <copyright></copyright>
    <lastBuildDate>Thu, 16 Nov 2017 00:00:00 +0000</lastBuildDate><atom:link href="https://nickyreinert.de/tags/messung/index.xml" rel="self" type="application/rss+xml" /><item>
      <title>Download-Geschwindigkeit messen und in Google DataStudio darstellen</title>
      <link>https://nickyreinert.de/2017/2017-11-16-download-geschwindigkeit-messen-und-in-googledata-studio-darstellen/</link>
      <pubDate>Thu, 16 Nov 2017 00:00:00 +0000</pubDate>
      <author></author>
      <guid>https://nickyreinert.de/2017/2017-11-16-download-geschwindigkeit-messen-und-in-googledata-studio-darstellen/</guid>
      <description>Alles beginnt mit einer fixen Idee. Meine war es, die Download-Geschwindigkeit meines Internet-Anschlusses zu messen. Doch das ist nur aussagekräftig, wenn man …</description>
      
      
      <content:encoded>&lt;![CDATA[
        
        <div class="ai-summary">
          <h3>AI-Zusammenfassung</h3>
          <p>Dieser Artikel beschreibt ein Projekt zur automatisierten Messung der Internet-Download-Geschwindigkeit und deren Visualisierung in Google Data Studio. Er erläutert ein Shell-Skript für wiederkehrende Downloads und Geschwindigkeitsberechnungen sowie die Entwicklung eines Google Data Studio Community Connectors zur Integration der Messdaten aus CSV-Dateien, die über Google Drive synchronisiert werden.</p>
          
          
          <p><strong>Hauptthemen:</strong> Internet Performance, Download Speed, Google Data Studio, Shell Scripting, Data Visualization, Google Apps Script, Automation, Monitoring</p>
          
          
          
          <p><strong>Schwierigkeitsgrad:</strong> advanced</p>
          
        </div>
        
        
        <p>Alles beginnt mit einer fixen Idee. Meine war es, die Download-Geschwindigkeit meines Internet-Anschlusses zu messen. Doch das ist nur aussagekräftig, wenn man es regelmäßig macht. Und um das ganze abzurunden, sollte man die Ergebnisse doch irgendwie noch in einem bunten Diagramm darstellen können. Aus der Idee wurde also ein Plan und schließlich ein Projekt.</p>
<p>Die Aufgabe lautet also: Ein Script soll in regelmäßigen Abständen Test-Dateien herunterladen. Die Dauer dafür und der Zeitpunkt des Tests schreibe ich in eine CSV-Datei, die mit GoogleDrive synchronisiert ist. Von dort werden die Ergebnisse im DataStudio von Google automatisch ausgelesen.</p>
<h2 id="schritt-1---das-automatisierte-download-script">Schritt 1 - das automatisierte Download-Script</h2>
<p>Damit der Test möglichst unterschiedliche Szenarien abdeckt, wollte ich nicht nur eine sondern mehrere Dateien verschiedener Größe anbieten. Dazu habe ich auf http://speedtest.ftp.otenet.gr zurückgegriffen. Dort werden verschiedene Dateigrößen zum Download angeboten.</p>
<p>Außerdem will ich jede Datei mehr als ein mal herunterladen und schließlich nach jedem Download eine Pause einlegen.</p>
<p>Die Hauptfunktionen möchte ich kurz erläutern:</p>
<ul>
<li>In den ersten beiden Schleife wird jeder Eintrag aus <strong>sourceFileSizes</strong> einmal durchlaufen und zwar so oft, wie mit <strong>loops</strong> festgelegt:</li>
</ul>
<p>for sourceFileSize in &ldquo;${sourceFileSizes[@]}&rdquo;;
do
while [ $i -lt $loops ]; do
&hellip;
done
done</p>
<ul>
<li>Innerhalb der Schleife wird im Grund nur die Test-Datei per wget an einen definierten Ort heruntergeladen. Außerdem soll natürlich die Dauer dafür gemessen werden. Das funktioniert ganz einfach und extrem präzise in Nanosekunden bzw. über den Unix-Timestamp.</li>
</ul>
<p>startTime=$(($(date +%s%N)))</p>
<p>wget &ndash;quiet
&ndash;output-document=$destinationFolder$sourceFileSize&quot;Mb.db.tmp&quot;
$sourceBaseUrl$sourceFileSize&quot;Mb.db&quot;</p>
<p>endTime=$(($(date +%s%N)))</p>
<p>delayNsecs=$(($endTime - $startTime))</p>
<ul>
<li>Die Berechnung der Geschwindigkeit ist etwas kompliziert, da ich auf der Shell nicht ohne weiteres Dezimalzahlen (float numbers) verarbeiten kann. Ich muss also mit <strong>awk</strong> arbeiten. Awk hingegen greift nicht auf die lokalen Variablen zu. Diese muss ich mit dem Parameter -v erst explizit übergeben. Am Ende entstehen dann Zeile wie diese, die im Grunde nur MByte in MBit umrechnen (mit 8 multiplizieren) und dann durch die Dauer in Sekunden dividieren. Das Ergebnis ist dann die Geschwindigkeit <strong>MBit/Sekunde</strong> - und damit perfekt vergleichbar mit der versprochenen Geschwindigkeit des Anbieters.</li>
</ul>
<p>MBitPerSec=
$(awk
-v sourceFileSize=$sourceFileSize
-v delayMSecs=$delaySecs
&lsquo;BEGIN{printf &ldquo;%.4f\n&rdquo;, ( sourceFileSize * 8 / delayMSecs)}&rsquo;)</p>
<ul>
<li>Schließlich wird der ganze Spaß natürlich noch in die CSV-Datei geschrieben:</li>
</ul>
<p>echo &ldquo;$(date &lsquo;+%Y-%m-%d %H:%M:%S&rsquo;),
$delayNsecs,
$delaySecs,
$MBitPerSec,
$sourceFileSize&quot;MByte&rdquo;&quot;
&raquo; $journalFile</p>
<ul>
<li>Am Ende wird entweder das Script verlassen, wenn zuletzt die 1GByte-Datei heruntergeladen wurde. Da das Script mehrmals am Tag läuft, will ich das Volumen nicht unnötig strapazieren. Oder es wird eine definierte Pause eingelegt, damit sich die Leitung abkühlen kann.</li>
</ul>
<p>Das komplette Script gibt es auf <a href="https://github.com/nickyreinert/speedTest/blob/master/speedTest.sh">github.</a></p>
<p>Das ganze muss nun nur in der CronTabelle des Systems regelmäßig aufgerufen werden. Der Zielordner</p>
<p>/share/Download/Speedtest/</p>
<p>wird außerdem mit GoogleDrive synchronisiert.</p>
<p>Die fertige Datei besitzt fünf Spalten, die den Zeitpunkt, die Dauer und die heruntergeladene Datei beinhalten:</p>
<p>column1,column2,column3,column4,column5
2017-10-31 11:48:36,1072562724,1.072562724,7.45877124105611,1MByte
2017-10-31 11:48:37,899356112,0.899356112,8.89525282950432,1MByte
2017-10-31 11:48:38,1002897956,1.002897956,7.97688334305469,1MByte</p>
<p>Anmerkung: Ich nutze den Netzwerkspeicher von QNAP, das TS-431. Für diesen wird ein Backup &amp; Sync-Plugin angeboten, das lokale Ordner mit einem Ordner in GoogleDrive synchronisiert.</p>
<h2 id="schritt-2---der-community-connector-für-das-datastudio">Schritt 2 - der Community Connector für das DataStudio</h2>
<p>Das Google DataStudio bringt von Hause aus schon eine Reihe von Schnittstellen mit, über die es möglich ist, auf z.B. Datenbanken oder Online-Dienste zurückzugreifen um in Echzeit an allerlei Messreihen zu kommen. Doch leider fehlt hier bisher ein Verbindung zu CSV-Dateien. Den Connector musste ich mir also erst selber erstellen.</p>
<p>Der Connector ist im Moment noch sehr einfach gehalten. Es ermöglicht keine großen Anpassungen und ist sicherlich noch verbesserungsfähig (<a href="https://datastudio.google.com/datasources/create?connectorId=AKfycbxxafV6ymAs6S2DRADTGKzJ2aNCdwrnMgRIMt-KJAzoO-YESKc19U9z2w">Link zum Connector</a>):</p>
<p>function getConfig() {
var config = {
configParams: [
{
type: &ldquo;INFO&rdquo;,
name: &ldquo;csvConnector&rdquo;,
text: &ldquo;The CSV-Connector currently supports a fixed amount of three columns. Name them column1, column2 and column3. Column1 is the dimension, column2 holds the metrics and column3 may be used as an additional category. You may change the label in the next window.&rdquo;
}
,{
type: &ldquo;TEXTINPUT&rdquo;,
name: &ldquo;url&rdquo;,
helpText: &ldquo;If you want to use a CSV-file from GoogleDrive, use this format where 123 at the end is your document id: <a href="https://drive.google.com/uc?export=download&amp;id=123%22">https://drive.google.com/uc?export=download&id=123"</a>,
displayName: &ldquo;Provide the url to your csv file.&rdquo;
}</p>
<pre><code>\]
</code></pre>
<p>};
return config;</p>
<p>};</p>
<p>var csvDataSchema = [
{
name: &lsquo;column1&rsquo;,
label: &lsquo;column1&rsquo;,
dataType: &lsquo;STRING&rsquo;,
semantics: {
conceptType: &lsquo;DIMENSION&rsquo;
}
},
{
name: &lsquo;column2&rsquo;,
label: &lsquo;column2&rsquo;,
dataType: &lsquo;NUMBER&rsquo;,
semantics: {
&ldquo;isReaggregatable&rdquo;: true,
conceptType: &lsquo;METRIC&rsquo;
}
},{
name: &lsquo;column3&rsquo;,
label: &lsquo;column3&rsquo;,
dataType: &lsquo;STRING&rsquo;,
semantics: {
&ldquo;isReaggregatable&rdquo;: false,
conceptType: &lsquo;DIMENSION&rsquo;
}</p>
<p>}
];</p>
<p>function getSchema(request) {</p>
<p>return {schema: csvDataSchema};</p>
<p>};</p>
<p>function isAdminUser() {
return true;
}</p>
<p>function csvToObject(array) {</p>
<p>var headers = array[0];</p>
<p>var jsonData = [];
for ( var i = 1, length = array.length; i &lt; length; i++ )
{
var row = array[i];
var data = {};
for ( var x = 0; x &lt; row.length; x++ )
{
data[headers[x]] = row[x];
}
jsonData.push(data);</p>
<pre><code>}

return jsonData;
</code></pre>
<p>}
/*
function stringToObject(string, separator)
{
var object = {};</p>
<p>var array = string.split(separator);</p>
<p>for (var i = 0; i &lt; array.length; i++) {</p>
<pre><code>if (i % 2 === 0) { 

  object\[array\[i\]\] = array\[i + 1\];

} else { 

  continue;
  
}
</code></pre>
<p>}</p>
<p>return object
}
*/</p>
<p>function getData(request) {</p>
<p>/*
I DONT GET SPLIT TO WORK SO FOR NOW THIS ONLY SUPPORTS PREPARED AND WORKING SHARING URL
FOR GOOGLE DRIVE
if (request.configParams.isGoogleDrive == &ldquo;true&rdquo;)
{
var urlString = request.configParams.url.toString();</p>
<pre><code>var urlArray = urlString.split(&quot;?&quot;);

  var params = stringToObject(urlArray, '&quot;');

  var docId = params\[&quot;id&quot;\];

  var url = &quot;https://drive.google.com/uc?export=download&amp;id=&quot; + docId;
</code></pre>
<p>} else {</p>
<pre><code>  var url = request.configParams.url;
</code></pre>
<p>}
*/</p>
<p>var url = request.configParams.url;</p>
<p>var dataSchema = [];</p>
<p>request.fields.forEach(function(field) {
for (var i = 0; i &lt; csvDataSchema.length; i++) {
if (csvDataSchema[i].name === field.name) {
dataSchema.push(csvDataSchema[i]);
break;
}
}
});</p>
<p>var csvFile = UrlFetchApp.fetch(url);</p>
<p>var csvData = Utilities.parseCsv(csvFile);</p>
<p>var sourceData = csvToObject(csvData);</p>
<p>var data = [];</p>
<p>sourceData.forEach(function(row) {
var values = [];
dataSchema.forEach(function(field) {
switch(field.name) {
case &lsquo;column1&rsquo;:
values.push(row.column1);
break;
case &lsquo;column2&rsquo;:
values.push(row.column2);
break;
case &lsquo;column3&rsquo;:
values.push(row.column3);
break;
default:
values.push(&rsquo;&rsquo;);
}
});
data.push({
values: values
});
});</p>
<p>return {
schema: dataSchema,
rows: data
};</p>
<p>};</p>
<p>function getAuthType() {
var response = {
&ldquo;type&rdquo;: &ldquo;NONE&rdquo;
};
return response;
}</p>
<p> </p>
<p>Leider unterstützt der Connector bisher nur drei Spalten mit vorgegebene Spalten-Namen. In einer nächsten Version sollte der Connector die Datei bereits im Vorfeld auslesen um die Spalten-Konfiguration selber zu erkennen.</p>
<p> </p>
<h2 id="schritt-3---darstellung-im-datastudio">Schritt 3 - Darstellung im DataStudio</h2>
<p>Im Data Studio ein Diagramm erstellen, dass die Messwerte der regelmäßigen Test-Download darstellt: Darum muss ich mich noch kümmern.</p>
<p> </p>
<h2 id="links-und-noch-mehr-links">Links und noch  mehr Links</h2>
<p><a href="https://github.com/nickyreinert/speedTest/blob/master/speedTest.sh">Link zu GitHub mit dem Quellcode</a></p>
<p><a href="https://datastudio.google.com/datasources/create?connectorId=AKfycbxxafV6ymAs6S2DRADTGKzJ2aNCdwrnMgRIMt-KJAzoO-YESKc19U9z2w">Link zum Connector</a></p>
<p><a href="https://developers.google.com/datastudio/connector/get-started">Einführung und Doku auf developers.google.com</a></p>
<p><a href="https://www.benlcollins.com/data-studio/community-connector/">kleinere Beispiele auf benlcollins.com</a></p>
<p><a href="https://github.com/google/datastudio">andere Projekte auf github.com</a></p>

        
        
        <div class="tags">
          <p><strong>Tags:</strong> apache, htacces, messung, php, redir, weiterleitung, Google Data Studio, Performance</p>
        </div>
        
      ]]></content:encoded>
      
      
      
      <category>development</category>
      
      <category>anleitungen</category>
      
      
      
      
      <media:content url="https://nickyreinert.de/images/posts/placeholder.jpg" type="image/jpeg">
        <media:title>Download-Geschwindigkeit messen und in Google DataStudio darstellen - Titelbild</media:title>
      </media:content>
      
      
      
      
      <dc:subject>Lesezeit: 5 Minuten</dc:subject>
      
      
      
      <dc:type>project_showcase</dc:type>
      
      
    </item><item>
      <title>Wie wirken sich viele 301 Weiterleitungen auf die  Performance aus?</title>
      <link>https://nickyreinert.de/2017/2017-10-26-wie-wirken-sich-viele-301-weiterleitungen-auf-die-performance-aus/</link>
      <pubDate>Thu, 26 Oct 2017 00:00:00 +0000</pubDate>
      <author></author>
      <guid>https://nickyreinert.de/2017/2017-10-26-wie-wirken-sich-viele-301-weiterleitungen-auf-die-performance-aus/</guid>
      <description>Um Weiterleitungen kommt man fast nicht herum. Vor allem im Bereich der Suchmaschinenoptimierung (SEO) sollte man vermeiden, dass es auf der Website zu …</description>
      
      
      <content:encoded>&lt;![CDATA[
        
        <div class="ai-summary">
          <h3>AI-Zusammenfassung</h3>
          <p>Dieser Artikel untersucht den Einfluss einer großen Anzahl von 301-Weiterleitungen, die über &#39;.htaccess&#39;-Dateien verwaltet werden, auf die Website-Performance. Basierend auf Messungen mit einem PHP-Skript wird gezeigt, dass die Antwortzeiten bei Zehntausenden von Weiterleitungen erheblich ansteigen, und es werden alternative Strategien zur effizienteren Verwaltung von Weiterleitungen vorgeschlagen.</p>
          
          
          <p><strong>Hauptthemen:</strong> SEO, Web Performance, 301 Redirects, .htaccess, Apache, PHP, Website Optimierung, HTTP Status Codes</p>
          
          
          
          <p><strong>Schwierigkeitsgrad:</strong> intermediate</p>
          
        </div>
        
        
        <p>Um Weiterleitungen kommt man fast nicht herum. Vor allem im Bereich der Suchmaschinenoptimierung (SEO) sollte man vermeiden, dass es auf der Website zu 404-Fehlern kommt - also Ressourcen, die nicht (mehr) vorhanden sind.</p>
<p>Ein Weg, um das zu beheben, ist die Einrichtung einer Weiterleitung von der alten, nicht mehr vorhandenen Ressource auf die neue Ressource.  Der HTTP-Statuscode dafür ist entweder 301 (temporär) oder 302 (für eine dauerhafte Weiterleitung). Weiterleitungen können z.B. mit einer .htaccess Datei eingerichtet werden. Dazu aktiviert man zunächst die sogenannte RewriteEngine, um eine URL zu einer anderen URL &ldquo;weiterzuleiten&rdquo;. Dann kann man beliebig viele, Regeln nach folgendem Prinzip festlegen (freilich gibt es noch weitaus mehr Möglichkeiten, wie z.B. reguläre Ausdrücke):</p>
<p>RewriteEngine On
Redirect 302 /redirect0r/foobar/1/ /redirect0r/foobar/index.php
Redirect 302 /redirect0r/foobar/2/ /redirect0r/foobar/index.php</p>
<p>Wenn man nun von einem System (z.B. Joomla) auf ein anderes System (z.B. Wordpress) umzieht und dabei nicht nur auf eine veraltete URL sondern auf 100 URLs stößt, die im neuen System nicht mehr existieren, stellt sich oft die Frage: Was mache ich mit all den alten URLs, die ich im neuen System nicht exakt abbilden kann? Macht es Sinn, die htaccess-Datei mit zahllosen Weiterleitungen zu überfluten? Es kommen zu ersten Zweifeln: Die htaccess-Datei wird bei jedem Aufruf der Website geladen. Kann eine große htaccess-Datei sich also negativ auf die Performance der Seite auswirken?</p>
<p>Die Frage hat auch mich beschäftigt und deshalb habe ich ein kleines PHP-Script geschrieben, dass helfen soll, die Antwort zu finden.</p>
<h2 id="funktion">Funktion</h2>
<p>Der Quellcode für das PHP-Script ist über <a href="https://github.com/nickyreinert/redirect0r">github</a> verfügbar. Das Script kann im Browser oder über die Kommandozeile aufgerufen werden. Sämtliche Einstellungen werden in einer JSON-Datei vorgenommen.</p>
<p>Der Ablauf des Scripts ist relativ einfach. In einer Schleife schreibt es eine beliebige Anzahl von Weiterleitungs-Regeln in eine htaccess-Datei. Diese Regeln haben folgenden Aufbau:</p>
<p>Redirect 302 /foobar/<strong><em>i</em></strong>/ /foobar/index.php</p>
<p>/foobar ist der Ordner, der für die Messung verwendet wird. In diesem Ordner befindet sich auch die htaccess-Datei. i ist eine fortlaufende Ziffer, die mit jedem Schleifendurchlauf inkrementiert wird. Schließlich wird das Ziel der Weiterleitung mit /foobar/index.php angegeben. Der Ordner und die Zieldatei sowie der Inhalt der Zieldatei können angepasst werden.</p>
<p>Nicht jeder Schleifendurchlauf schreibt in die htaccess-Datei und ruft Test-URL sofort auf. Das geschieht in definierbaren Abschnitten. Diese Schrittweite ist definierbar. Die Zeit für den Aufruf wird schließlich gemessen .</p>
<p>Weiterhin ist es möglich, die Aufrufe innerhalb einer Schrittweite zu wiederholen, also mehrere Abfragen nacheinander, um z.B. in der späteren Auswertung aus den Ergebnissen einen Mittelwert zu errechnen. Nach jeder Abfrage kann außerdem eine cool-down-Phase stattfinden, bevor der nächste Abruf stattfindet..</p>
<h2 id="messergebnisse">Messergebnisse</h2>
<p>Die Messergebnisse offenbaren keine Überraschung. Je größer die Datei, desto größer die Antwortzeiten.  Im Detail heißt das:</p>
<p>Gemessen wurden mit drei verschiedenen Methoden:</p>
<ul>
<li>auf einem lokalen Webserver (MacBook Pro mit 16 GByte RAM und 2,7 GHz i5) per Aufruf im Browser (local server)</li>
<li>auf einem gehosteten Webserver (unbekannte Hardware) per Aufruf im Browser (remote server)</li>
<li>auf einem gehosteten Webserver per Aufruf auf der Kommandozeile (remote server CLI)</li>
</ul>
<p>Als Zeilenlimitin der htaccess-Datei wurden 100.000 Zeilen gewählt. Die Schrittweite beträgt 5.000. Nach jedem Aufruf gab es eine Pause von 3 Sekunden. Insgesamt gab es drei Aufrufe je Zeilenanzahl. Über diese 3 Aufrufe zu einer bestimmten Zeilenanzahl wurde schließlich der Mittelwert errechnet.</p>
<p>Bei 10.000 Zeilen wurde bei allen Methoden ca. 90 ms gemessen. Bei 20.000 Zeilen in der htaccess-Datei beträgt die Reaktionszeiten knapp das doppelte, aber immer noch recht unauffällige 185 ms. Ab da gehen die gemessenen Zeit leicht auseinander, der lokale Server scheint die schlechtere Performance zu haben.</p>
<p>Bei 60.000 Zeilen wird bei allen Methoden die &ldquo;magische&rdquo; Grenze von 1 Sekunde überschritten. Die Antwortzeiten steigen jetzt nicht mehr proportional. Bei 100.000 Zeilen benötigt der lokale Server schon über 4 Sekunden für die Antwort. Der gehostete Webserver braucht dafür über knapp 3 Sekunden.</p>
<p>[caption id=&ldquo;attachment_1580&rdquo; align=&ldquo;aligncenter&rdquo; width=&ldquo;300&rdquo;]<a href="https://www.nickyreinert.de/files/wie-wirken-sich-viele-301-weiterleitungen-auf-die-performance-aus/htaccess-geschwindigkeit.png"><img src="/2017/2017-10-26-wie-wirken-sich-viele-301-weiterleitungen-auf-die-performance-aus/images/htaccess-geschwindigkeit-300x186.png" alt="Geschwindigkeit bei wachsender htaccess-Datei"></a> Geschwindigkeit bei wachsender htaccess-Datei[/caption]</p>
<h2 id="fazit">Fazit</h2>
<p>Natürlich wirkt sich eine große htaccess-Datei auf die Performance des Servers auf. Denn wie bereits bemerkt, muss der Webserver diese Datei bei jeder Anfrage öffnen und verarbeiten. Allerdings ist der negative Einfluss ziemlich gering und macht sich erst bei einer sehr großen Anzahl von Zeilen bemerkbar.</p>
<p>Eine htaccess-Datei mit 10.000 Zeilen verringert die Antwortzeit kaum. Allerdings steigt die Antwortzeit überproportional an. Bei 100.000 Zeilen ist sie bereits 50 mal langsamer.</p>
<p>Im Bereich ab 40.000 Zeilen dürfte der Einfluss auch nach außen hin spürbar sein. Natürlich hängt auch diese Erkenntnis stark von der verwendeten Hardware ab: Der lokale Webserver ist bei meinen Messungen etwas langsamer als der Server des Hosters.</p>
<p>Wer eine htaccess-Datei mit 10.000 Weiterleitungen pflegt sollte grundsätzlich sein Konzept überdenken. Oft lässt sich das entweder durch einen sauberen Umzug bzw. eine Anpassung des neuen Systems oder durch Weiterleitungen mit regulären Ausdrücken besser lösen. Am besten ist es natürlich, wenn man gar nicht erst in die Verlegenheit kommt, Weiterleitungen nutzen zu müssen.</p>

        
        
        <div class="tags">
          <p><strong>Tags:</strong> apache, htacces, messung, php, redir, weiterleitung, Web Performance</p>
        </div>
        
      ]]></content:encoded>
      
      
      
      <category>hosting</category>
      
      <category>anleitungen</category>
      
      
      
      
      <media:content url="https://nickyreinert.de/images/posts/placeholder.jpg" type="image/jpeg">
        <media:title>Wie wirken sich viele 301 Weiterleitungen auf die  Performance aus? - Titelbild</media:title>
      </media:content>
      
      
      
      
      <dc:subject>Lesezeit: 5 Minuten</dc:subject>
      
      
      
      <dc:type>performance_analysis</dc:type>
      
      
    </item>
  </channel>
</rss>