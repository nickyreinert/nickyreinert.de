<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" 
     xmlns:atom="http://www.w3.org/2005/Atom"
     xmlns:content="http://purl.org/rss/1.0/modules/content/"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:media="http://search.yahoo.com/mrss/">
  <channel>
    <title>Weiterleitung auf Nicky Reinert</title>
    <link>http://localhost:1313/tags/weiterleitung/</link>
    <description>Blog &amp; Projekte von Nicky Reinert (Institut für digitale Herausforderungen): Webentwicklung &amp; Software Development, SEO &amp; Analytics, Hosting &amp; DevOps, WordPress &amp; Hugo, Tools &amp; Projekte, Datenschutz und digitale Kultur – plus Texte zu KI sowie Autismus &amp; Gesellschaft.</description>
    <generator>Hugo 0.148.2</generator>
    <language>de</language>
    <managingEditor></managingEditor>
    <webMaster></webMaster>
    <copyright></copyright>
    <lastBuildDate>Thu, 16 Nov 2017 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/tags/weiterleitung/index.xml" rel="self" type="application/rss+xml" /><item>
      <title>Download-Geschwindigkeit messen und in Google DataStudio darstellen</title>
      <link>http://localhost:1313/2017/2017-11-16-download-geschwindigkeit-messen-und-in-googledata-studio-darstellen/</link>
      <pubDate>Thu, 16 Nov 2017 00:00:00 +0000</pubDate>
      <author></author>
      <guid>http://localhost:1313/2017/2017-11-16-download-geschwindigkeit-messen-und-in-googledata-studio-darstellen/</guid>
      <description>Alles beginnt mit einer fixen Idee. Meine war es, die Download-Geschwindigkeit meines Internet-Anschlusses zu messen. Doch das ist nur aussagekräftig, wenn man …</description>
      
      
      <content:encoded>&lt;![CDATA[
        
        <div class="ai-summary">
          <h3>AI-Zusammenfassung</h3>
          <p>Dieser Artikel beschreibt ein Projekt zur automatisierten Messung der Internet-Download-Geschwindigkeit und deren Visualisierung in Google Data Studio. Er erläutert ein Shell-Skript für wiederkehrende Downloads und Geschwindigkeitsberechnungen sowie die Entwicklung eines Google Data Studio Community Connectors zur Integration der Messdaten aus CSV-Dateien, die über Google Drive synchronisiert werden.</p>
          
          
          <p><strong>Hauptthemen:</strong> Internet Performance, Download Speed, Google Data Studio, Shell Scripting, Data Visualization, Google Apps Script, Automation, Monitoring</p>
          
          
          
          <p><strong>Schwierigkeitsgrad:</strong> advanced</p>
          
        </div>
        
        
        <p>Alles beginnt mit einer fixen Idee. Meine war es, die Download-Geschwindigkeit meines Internet-Anschlusses zu messen. Doch das ist nur aussagekräftig, wenn man es regelmäßig macht. Und um das ganze abzurunden, sollte man die Ergebnisse doch irgendwie noch in einem bunten Diagramm darstellen können. Aus der Idee wurde also ein Plan und schließlich ein Projekt.</p>
<p>Die Aufgabe lautet also: Ein Script soll in regelmäßigen Abständen Test-Dateien herunterladen. Die Dauer dafür und der Zeitpunkt des Tests schreibe ich in eine CSV-Datei, die mit GoogleDrive synchronisiert ist. Von dort werden die Ergebnisse im DataStudio von Google automatisch ausgelesen.</p>
<h2 id="schritt-1---das-automatisierte-download-script">Schritt 1 - das automatisierte Download-Script</h2>
<p>Damit der Test möglichst unterschiedliche Szenarien abdeckt, wollte ich nicht nur eine sondern mehrere Dateien verschiedener Größe anbieten. Dazu habe ich auf http://speedtest.ftp.otenet.gr zurückgegriffen. Dort werden verschiedene Dateigrößen zum Download angeboten.</p>
<p>Außerdem will ich jede Datei mehr als ein mal herunterladen und schließlich nach jedem Download eine Pause einlegen.</p>
<p>Die Hauptfunktionen möchte ich kurz erläutern:</p>
<ul>
<li>In den ersten beiden Schleife wird jeder Eintrag aus <strong>sourceFileSizes</strong> einmal durchlaufen und zwar so oft, wie mit <strong>loops</strong> festgelegt:</li>
</ul>
<p>for sourceFileSize in &ldquo;${sourceFileSizes[@]}&rdquo;;
do
while [ $i -lt $loops ]; do
&hellip;
done
done</p>
<ul>
<li>Innerhalb der Schleife wird im Grund nur die Test-Datei per wget an einen definierten Ort heruntergeladen. Außerdem soll natürlich die Dauer dafür gemessen werden. Das funktioniert ganz einfach und extrem präzise in Nanosekunden bzw. über den Unix-Timestamp.</li>
</ul>
<p>startTime=$(($(date +%s%N)))</p>
<p>wget &ndash;quiet
&ndash;output-document=$destinationFolder$sourceFileSize&quot;Mb.db.tmp&quot;
$sourceBaseUrl$sourceFileSize&quot;Mb.db&quot;</p>
<p>endTime=$(($(date +%s%N)))</p>
<p>delayNsecs=$(($endTime - $startTime))</p>
<ul>
<li>Die Berechnung der Geschwindigkeit ist etwas kompliziert, da ich auf der Shell nicht ohne weiteres Dezimalzahlen (float numbers) verarbeiten kann. Ich muss also mit <strong>awk</strong> arbeiten. Awk hingegen greift nicht auf die lokalen Variablen zu. Diese muss ich mit dem Parameter -v erst explizit übergeben. Am Ende entstehen dann Zeile wie diese, die im Grunde nur MByte in MBit umrechnen (mit 8 multiplizieren) und dann durch die Dauer in Sekunden dividieren. Das Ergebnis ist dann die Geschwindigkeit <strong>MBit/Sekunde</strong> - und damit perfekt vergleichbar mit der versprochenen Geschwindigkeit des Anbieters.</li>
</ul>
<p>MBitPerSec=
$(awk
-v sourceFileSize=$sourceFileSize
-v delayMSecs=$delaySecs
&lsquo;BEGIN{printf &ldquo;%.4f\n&rdquo;, ( sourceFileSize * 8 / delayMSecs)}&rsquo;)</p>
<ul>
<li>Schließlich wird der ganze Spaß natürlich noch in die CSV-Datei geschrieben:</li>
</ul>
<p>echo &ldquo;$(date &lsquo;+%Y-%m-%d %H:%M:%S&rsquo;),
$delayNsecs,
$delaySecs,
$MBitPerSec,
$sourceFileSize&quot;MByte&rdquo;&quot;
&raquo; $journalFile</p>
<ul>
<li>Am Ende wird entweder das Script verlassen, wenn zuletzt die 1GByte-Datei heruntergeladen wurde. Da das Script mehrmals am Tag läuft, will ich das Volumen nicht unnötig strapazieren. Oder es wird eine definierte Pause eingelegt, damit sich die Leitung abkühlen kann.</li>
</ul>
<p>Das komplette Script gibt es auf <a href="https://github.com/nickyreinert/speedTest/blob/master/speedTest.sh">github.</a></p>
<p>Das ganze muss nun nur in der CronTabelle des Systems regelmäßig aufgerufen werden. Der Zielordner</p>
<p>/share/Download/Speedtest/</p>
<p>wird außerdem mit GoogleDrive synchronisiert.</p>
<p>Die fertige Datei besitzt fünf Spalten, die den Zeitpunkt, die Dauer und die heruntergeladene Datei beinhalten:</p>
<p>column1,column2,column3,column4,column5
2017-10-31 11:48:36,1072562724,1.072562724,7.45877124105611,1MByte
2017-10-31 11:48:37,899356112,0.899356112,8.89525282950432,1MByte
2017-10-31 11:48:38,1002897956,1.002897956,7.97688334305469,1MByte</p>
<p>Anmerkung: Ich nutze den Netzwerkspeicher von QNAP, das TS-431. Für diesen wird ein Backup &amp; Sync-Plugin angeboten, das lokale Ordner mit einem Ordner in GoogleDrive synchronisiert.</p>
<h2 id="schritt-2---der-community-connector-für-das-datastudio">Schritt 2 - der Community Connector für das DataStudio</h2>
<p>Das Google DataStudio bringt von Hause aus schon eine Reihe von Schnittstellen mit, über die es möglich ist, auf z.B. Datenbanken oder Online-Dienste zurückzugreifen um in Echzeit an allerlei Messreihen zu kommen. Doch leider fehlt hier bisher ein Verbindung zu CSV-Dateien. Den Connector musste ich mir also erst selber erstellen.</p>
<p>Der Connector ist im Moment noch sehr einfach gehalten. Es ermöglicht keine großen Anpassungen und ist sicherlich noch verbesserungsfähig (<a href="https://datastudio.google.com/datasources/create?connectorId=AKfycbxxafV6ymAs6S2DRADTGKzJ2aNCdwrnMgRIMt-KJAzoO-YESKc19U9z2w">Link zum Connector</a>):</p>
<p>function getConfig() {
var config = {
configParams: [
{
type: &ldquo;INFO&rdquo;,
name: &ldquo;csvConnector&rdquo;,
text: &ldquo;The CSV-Connector currently supports a fixed amount of three columns. Name them column1, column2 and column3. Column1 is the dimension, column2 holds the metrics and column3 may be used as an additional category. You may change the label in the next window.&rdquo;
}
,{
type: &ldquo;TEXTINPUT&rdquo;,
name: &ldquo;url&rdquo;,
helpText: &ldquo;If you want to use a CSV-file from GoogleDrive, use this format where 123 at the end is your document id: <a href="https://drive.google.com/uc?export=download&amp;id=123%22">https://drive.google.com/uc?export=download&id=123"</a>,
displayName: &ldquo;Provide the url to your csv file.&rdquo;
}</p>
<pre><code>\]
</code></pre>
<p>};
return config;</p>
<p>};</p>
<p>var csvDataSchema = [
{
name: &lsquo;column1&rsquo;,
label: &lsquo;column1&rsquo;,
dataType: &lsquo;STRING&rsquo;,
semantics: {
conceptType: &lsquo;DIMENSION&rsquo;
}
},
{
name: &lsquo;column2&rsquo;,
label: &lsquo;column2&rsquo;,
dataType: &lsquo;NUMBER&rsquo;,
semantics: {
&ldquo;isReaggregatable&rdquo;: true,
conceptType: &lsquo;METRIC&rsquo;
}
},{
name: &lsquo;column3&rsquo;,
label: &lsquo;column3&rsquo;,
dataType: &lsquo;STRING&rsquo;,
semantics: {
&ldquo;isReaggregatable&rdquo;: false,
conceptType: &lsquo;DIMENSION&rsquo;
}</p>
<p>}
];</p>
<p>function getSchema(request) {</p>
<p>return {schema: csvDataSchema};</p>
<p>};</p>
<p>function isAdminUser() {
return true;
}</p>
<p>function csvToObject(array) {</p>
<p>var headers = array[0];</p>
<p>var jsonData = [];
for ( var i = 1, length = array.length; i &lt; length; i++ )
{
var row = array[i];
var data = {};
for ( var x = 0; x &lt; row.length; x++ )
{
data[headers[x]] = row[x];
}
jsonData.push(data);</p>
<pre><code>}

return jsonData;
</code></pre>
<p>}
/*
function stringToObject(string, separator)
{
var object = {};</p>
<p>var array = string.split(separator);</p>
<p>for (var i = 0; i &lt; array.length; i++) {</p>
<pre><code>if (i % 2 === 0) { 

  object\[array\[i\]\] = array\[i + 1\];

} else { 

  continue;
  
}
</code></pre>
<p>}</p>
<p>return object
}
*/</p>
<p>function getData(request) {</p>
<p>/*
I DONT GET SPLIT TO WORK SO FOR NOW THIS ONLY SUPPORTS PREPARED AND WORKING SHARING URL
FOR GOOGLE DRIVE
if (request.configParams.isGoogleDrive == &ldquo;true&rdquo;)
{
var urlString = request.configParams.url.toString();</p>
<pre><code>var urlArray = urlString.split(&quot;?&quot;);

  var params = stringToObject(urlArray, '&quot;');

  var docId = params\[&quot;id&quot;\];

  var url = &quot;https://drive.google.com/uc?export=download&amp;id=&quot; + docId;
</code></pre>
<p>} else {</p>
<pre><code>  var url = request.configParams.url;
</code></pre>
<p>}
*/</p>
<p>var url = request.configParams.url;</p>
<p>var dataSchema = [];</p>
<p>request.fields.forEach(function(field) {
for (var i = 0; i &lt; csvDataSchema.length; i++) {
if (csvDataSchema[i].name === field.name) {
dataSchema.push(csvDataSchema[i]);
break;
}
}
});</p>
<p>var csvFile = UrlFetchApp.fetch(url);</p>
<p>var csvData = Utilities.parseCsv(csvFile);</p>
<p>var sourceData = csvToObject(csvData);</p>
<p>var data = [];</p>
<p>sourceData.forEach(function(row) {
var values = [];
dataSchema.forEach(function(field) {
switch(field.name) {
case &lsquo;column1&rsquo;:
values.push(row.column1);
break;
case &lsquo;column2&rsquo;:
values.push(row.column2);
break;
case &lsquo;column3&rsquo;:
values.push(row.column3);
break;
default:
values.push(&rsquo;&rsquo;);
}
});
data.push({
values: values
});
});</p>
<p>return {
schema: dataSchema,
rows: data
};</p>
<p>};</p>
<p>function getAuthType() {
var response = {
&ldquo;type&rdquo;: &ldquo;NONE&rdquo;
};
return response;
}</p>
<p> </p>
<p>Leider unterstützt der Connector bisher nur drei Spalten mit vorgegebene Spalten-Namen. In einer nächsten Version sollte der Connector die Datei bereits im Vorfeld auslesen um die Spalten-Konfiguration selber zu erkennen.</p>
<p> </p>
<h2 id="schritt-3---darstellung-im-datastudio">Schritt 3 - Darstellung im DataStudio</h2>
<p>Im Data Studio ein Diagramm erstellen, dass die Messwerte der regelmäßigen Test-Download darstellt: Darum muss ich mich noch kümmern.</p>
<p> </p>
<h2 id="links-und-noch-mehr-links">Links und noch  mehr Links</h2>
<p><a href="https://github.com/nickyreinert/speedTest/blob/master/speedTest.sh">Link zu GitHub mit dem Quellcode</a></p>
<p><a href="https://datastudio.google.com/datasources/create?connectorId=AKfycbxxafV6ymAs6S2DRADTGKzJ2aNCdwrnMgRIMt-KJAzoO-YESKc19U9z2w">Link zum Connector</a></p>
<p><a href="https://developers.google.com/datastudio/connector/get-started">Einführung und Doku auf developers.google.com</a></p>
<p><a href="https://www.benlcollins.com/data-studio/community-connector/">kleinere Beispiele auf benlcollins.com</a></p>
<p><a href="https://github.com/google/datastudio">andere Projekte auf github.com</a></p>

        
        
        <div class="tags">
          <p><strong>Tags:</strong> apache, htacces, messung, php, redir, weiterleitung, Google Data Studio, Performance</p>
        </div>
        
      ]]></content:encoded>
      
      
      
      <category>development</category>
      
      <category>anleitungen</category>
      
      
      
      
      <media:content url="http://localhost:1313/images/posts/placeholder.jpg" type="image/jpeg">
        <media:title>Download-Geschwindigkeit messen und in Google DataStudio darstellen - Titelbild</media:title>
      </media:content>
      
      
      
      
      <dc:subject>Lesezeit: 5 Minuten</dc:subject>
      
      
      
      <dc:type>project_showcase</dc:type>
      
      
    </item><item>
      <title>Wie wirken sich viele 301 Weiterleitungen auf die  Performance aus?</title>
      <link>http://localhost:1313/2017/2017-10-26-wie-wirken-sich-viele-301-weiterleitungen-auf-die-performance-aus/</link>
      <pubDate>Thu, 26 Oct 2017 00:00:00 +0000</pubDate>
      <author></author>
      <guid>http://localhost:1313/2017/2017-10-26-wie-wirken-sich-viele-301-weiterleitungen-auf-die-performance-aus/</guid>
      <description>Um Weiterleitungen kommt man fast nicht herum. Vor allem im Bereich der Suchmaschinenoptimierung (SEO) sollte man vermeiden, dass es auf der Website zu …</description>
      
      
      <content:encoded>&lt;![CDATA[
        
        <div class="ai-summary">
          <h3>AI-Zusammenfassung</h3>
          <p>Dieser Artikel untersucht den Einfluss einer großen Anzahl von 301-Weiterleitungen, die über &#39;.htaccess&#39;-Dateien verwaltet werden, auf die Website-Performance. Basierend auf Messungen mit einem PHP-Skript wird gezeigt, dass die Antwortzeiten bei Zehntausenden von Weiterleitungen erheblich ansteigen, und es werden alternative Strategien zur effizienteren Verwaltung von Weiterleitungen vorgeschlagen.</p>
          
          
          <p><strong>Hauptthemen:</strong> SEO, Web Performance, 301 Redirects, .htaccess, Apache, PHP, Website Optimierung, HTTP Status Codes</p>
          
          
          
          <p><strong>Schwierigkeitsgrad:</strong> intermediate</p>
          
        </div>
        
        
        <p>Um Weiterleitungen kommt man fast nicht herum. Vor allem im Bereich der Suchmaschinenoptimierung (SEO) sollte man vermeiden, dass es auf der Website zu 404-Fehlern kommt - also Ressourcen, die nicht (mehr) vorhanden sind.</p>
<p>Ein Weg, um das zu beheben, ist die Einrichtung einer Weiterleitung von der alten, nicht mehr vorhandenen Ressource auf die neue Ressource.  Der HTTP-Statuscode dafür ist entweder 301 (temporär) oder 302 (für eine dauerhafte Weiterleitung). Weiterleitungen können z.B. mit einer .htaccess Datei eingerichtet werden. Dazu aktiviert man zunächst die sogenannte RewriteEngine, um eine URL zu einer anderen URL &ldquo;weiterzuleiten&rdquo;. Dann kann man beliebig viele, Regeln nach folgendem Prinzip festlegen (freilich gibt es noch weitaus mehr Möglichkeiten, wie z.B. reguläre Ausdrücke):</p>
<p>RewriteEngine On
Redirect 302 /redirect0r/foobar/1/ /redirect0r/foobar/index.php
Redirect 302 /redirect0r/foobar/2/ /redirect0r/foobar/index.php</p>
<p>Wenn man nun von einem System (z.B. Joomla) auf ein anderes System (z.B. Wordpress) umzieht und dabei nicht nur auf eine veraltete URL sondern auf 100 URLs stößt, die im neuen System nicht mehr existieren, stellt sich oft die Frage: Was mache ich mit all den alten URLs, die ich im neuen System nicht exakt abbilden kann? Macht es Sinn, die htaccess-Datei mit zahllosen Weiterleitungen zu überfluten? Es kommen zu ersten Zweifeln: Die htaccess-Datei wird bei jedem Aufruf der Website geladen. Kann eine große htaccess-Datei sich also negativ auf die Performance der Seite auswirken?</p>
<p>Die Frage hat auch mich beschäftigt und deshalb habe ich ein kleines PHP-Script geschrieben, dass helfen soll, die Antwort zu finden.</p>
<h2 id="funktion">Funktion</h2>
<p>Der Quellcode für das PHP-Script ist über <a href="https://github.com/nickyreinert/redirect0r">github</a> verfügbar. Das Script kann im Browser oder über die Kommandozeile aufgerufen werden. Sämtliche Einstellungen werden in einer JSON-Datei vorgenommen.</p>
<p>Der Ablauf des Scripts ist relativ einfach. In einer Schleife schreibt es eine beliebige Anzahl von Weiterleitungs-Regeln in eine htaccess-Datei. Diese Regeln haben folgenden Aufbau:</p>
<p>Redirect 302 /foobar/<strong><em>i</em></strong>/ /foobar/index.php</p>
<p>/foobar ist der Ordner, der für die Messung verwendet wird. In diesem Ordner befindet sich auch die htaccess-Datei. i ist eine fortlaufende Ziffer, die mit jedem Schleifendurchlauf inkrementiert wird. Schließlich wird das Ziel der Weiterleitung mit /foobar/index.php angegeben. Der Ordner und die Zieldatei sowie der Inhalt der Zieldatei können angepasst werden.</p>
<p>Nicht jeder Schleifendurchlauf schreibt in die htaccess-Datei und ruft Test-URL sofort auf. Das geschieht in definierbaren Abschnitten. Diese Schrittweite ist definierbar. Die Zeit für den Aufruf wird schließlich gemessen .</p>
<p>Weiterhin ist es möglich, die Aufrufe innerhalb einer Schrittweite zu wiederholen, also mehrere Abfragen nacheinander, um z.B. in der späteren Auswertung aus den Ergebnissen einen Mittelwert zu errechnen. Nach jeder Abfrage kann außerdem eine cool-down-Phase stattfinden, bevor der nächste Abruf stattfindet..</p>
<h2 id="messergebnisse">Messergebnisse</h2>
<p>Die Messergebnisse offenbaren keine Überraschung. Je größer die Datei, desto größer die Antwortzeiten.  Im Detail heißt das:</p>
<p>Gemessen wurden mit drei verschiedenen Methoden:</p>
<ul>
<li>auf einem lokalen Webserver (MacBook Pro mit 16 GByte RAM und 2,7 GHz i5) per Aufruf im Browser (local server)</li>
<li>auf einem gehosteten Webserver (unbekannte Hardware) per Aufruf im Browser (remote server)</li>
<li>auf einem gehosteten Webserver per Aufruf auf der Kommandozeile (remote server CLI)</li>
</ul>
<p>Als Zeilenlimitin der htaccess-Datei wurden 100.000 Zeilen gewählt. Die Schrittweite beträgt 5.000. Nach jedem Aufruf gab es eine Pause von 3 Sekunden. Insgesamt gab es drei Aufrufe je Zeilenanzahl. Über diese 3 Aufrufe zu einer bestimmten Zeilenanzahl wurde schließlich der Mittelwert errechnet.</p>
<p>Bei 10.000 Zeilen wurde bei allen Methoden ca. 90 ms gemessen. Bei 20.000 Zeilen in der htaccess-Datei beträgt die Reaktionszeiten knapp das doppelte, aber immer noch recht unauffällige 185 ms. Ab da gehen die gemessenen Zeit leicht auseinander, der lokale Server scheint die schlechtere Performance zu haben.</p>
<p>Bei 60.000 Zeilen wird bei allen Methoden die &ldquo;magische&rdquo; Grenze von 1 Sekunde überschritten. Die Antwortzeiten steigen jetzt nicht mehr proportional. Bei 100.000 Zeilen benötigt der lokale Server schon über 4 Sekunden für die Antwort. Der gehostete Webserver braucht dafür über knapp 3 Sekunden.</p>
<p>[caption id=&ldquo;attachment_1580&rdquo; align=&ldquo;aligncenter&rdquo; width=&ldquo;300&rdquo;]<a href="https://www.nickyreinert.de/files/wie-wirken-sich-viele-301-weiterleitungen-auf-die-performance-aus/htaccess-geschwindigkeit.png"><img src="/2017/2017-10-26-wie-wirken-sich-viele-301-weiterleitungen-auf-die-performance-aus/images/htaccess-geschwindigkeit-300x186.png" alt="Geschwindigkeit bei wachsender htaccess-Datei"></a> Geschwindigkeit bei wachsender htaccess-Datei[/caption]</p>
<h2 id="fazit">Fazit</h2>
<p>Natürlich wirkt sich eine große htaccess-Datei auf die Performance des Servers auf. Denn wie bereits bemerkt, muss der Webserver diese Datei bei jeder Anfrage öffnen und verarbeiten. Allerdings ist der negative Einfluss ziemlich gering und macht sich erst bei einer sehr großen Anzahl von Zeilen bemerkbar.</p>
<p>Eine htaccess-Datei mit 10.000 Zeilen verringert die Antwortzeit kaum. Allerdings steigt die Antwortzeit überproportional an. Bei 100.000 Zeilen ist sie bereits 50 mal langsamer.</p>
<p>Im Bereich ab 40.000 Zeilen dürfte der Einfluss auch nach außen hin spürbar sein. Natürlich hängt auch diese Erkenntnis stark von der verwendeten Hardware ab: Der lokale Webserver ist bei meinen Messungen etwas langsamer als der Server des Hosters.</p>
<p>Wer eine htaccess-Datei mit 10.000 Weiterleitungen pflegt sollte grundsätzlich sein Konzept überdenken. Oft lässt sich das entweder durch einen sauberen Umzug bzw. eine Anpassung des neuen Systems oder durch Weiterleitungen mit regulären Ausdrücken besser lösen. Am besten ist es natürlich, wenn man gar nicht erst in die Verlegenheit kommt, Weiterleitungen nutzen zu müssen.</p>

        
        
        <div class="tags">
          <p><strong>Tags:</strong> apache, htacces, messung, php, redir, weiterleitung, Web Performance</p>
        </div>
        
      ]]></content:encoded>
      
      
      
      <category>hosting</category>
      
      <category>anleitungen</category>
      
      
      
      
      <media:content url="http://localhost:1313/images/posts/placeholder.jpg" type="image/jpeg">
        <media:title>Wie wirken sich viele 301 Weiterleitungen auf die  Performance aus? - Titelbild</media:title>
      </media:content>
      
      
      
      
      <dc:subject>Lesezeit: 5 Minuten</dc:subject>
      
      
      
      <dc:type>performance_analysis</dc:type>
      
      
    </item><item>
      <title>Welche Möglichkeiten habe ich um Weiterleitungen anzulegen?</title>
      <link>http://localhost:1313/2016/2016-09-25-welche-moeglichkeiten-habe-ich-um-weiterleitungen-anzulegen/</link>
      <pubDate>Sun, 25 Sep 2016 00:00:00 +0000</pubDate>
      <author></author>
      <guid>http://localhost:1313/2016/2016-09-25-welche-moeglichkeiten-habe-ich-um-weiterleitungen-anzulegen/</guid>
      <description>Was sind Weiterleitungen? Du hast von einem alten Shop oder Content Management System auf ein neues System gewechselt, willst eine neue Permalink-Struktur …</description>
      
      
      <content:encoded>&lt;![CDATA[
        
        <div class="ai-summary">
          <h3>AI-Zusammenfassung</h3>
          <p>Der Artikel erklärt die Notwendigkeit von 301-Weiterleitungen nach Website-Umzügen, um den SEO-Wert zu erhalten. Es werden zwei technische Methoden zur Implementierung vorgestellt: die manuelle Erstellung von Weiterleitungen mittels der PHP &#39;header()&#39;-Funktion und die dynamische Konfiguration über &#39;.htaccess&#39;-Regeln mit &#39;mod_rewrite&#39;.</p>
          
          
          <p><strong>Hauptthemen:</strong> SEO, Webserver-Konfiguration, Apache, .htaccess, PHP, Website-Migration</p>
          
          
          
          <p><strong>Schwierigkeitsgrad:</strong> intermediate</p>
          
        </div>
        
        
        <h2 id="was-sind-weiterleitungen">Was sind Weiterleitungen?</h2>
<p>Du hast von einem alten Shop oder Content Management System auf ein neues System gewechselt, willst eine neue Permalink-Struktur einführen oder hast einfach nur Inhalte verschoben? Das Problem in solchen Szenarien ist, dass die Verzeichnisstruktur nicht exakt übernommen werden kann. Doch keine Panik. Dass sich Inhalte und Verlinkungen im Internet ändern, gehört zum Konzept des WWW dazu. Wichtig ist, dass man diese Dynamik entsprechend berücksichtigt und dafür sorgt, dass eingehender Traffic auf die eigene Seite nicht auf die nicht-vorhandene Seiten stößt. Mit sogenannten Weiterleitungen.</p>
<h2 id="suchmaschinen-index-und-backlinks">Suchmaschinen-Index und Backlinks</h2>
<p>Zwei Arten von eingehenden Traffic kann man grundsätzlich unterscheiden: Erstens gibt es den Traffic der von einer Suchmaschine (<strong>organischer Traffic</strong>) kommt. Die Suchmaschine nutzt dafür einen Index, der sich aber nach bestimmten Regeln aktualisiert. Daneben gibt es Traffic über <strong>Backlinks</strong> von externen Seiten, die keine Suchmaschinen sind. Diese Backlinks sind natürlich auch veränderbar, aber, wenn man es so nennen will, redaktionell gepflegt. Streng genommen könnte man natürlich noch anderen Traffic unterscheiden, wie z.B. den aus der Suchmaschinen-Werbung (SEA). Doch da dieser Teil der eigenen Online-Marketing-Kampagne ist, versteht es sich von selber, dass die Links dort ordentlich gepflegt werden müssen.</p>
<h2 id="wann-benötige-ich-eine-weiterleitung">Wann benötige ich eine Weiterleitung?</h2>
<p>Die Suchmaschinen lassen sich eigentlich relativ unkompliziert mit einer aktuellen Sitemap befriedigen, die ja als aktuelles Verzeichnis der eigenen Seite. Google bietet in der Search Console sogar an, die Seite <a href="https://www.google.com/webmasters/tools/home?hl=de">neu zu indizieren</a>.</p>
<p>Doch was ist mit den andern Suchmaschinen und vor allem all den wertvollen Backlinks, die nicht mehr gültig sind? Nicht jeder eingehend Link lässt sich so einfach aktualisieren und führt im ungünstigen Fall zum HTTP-Fehlercode 404 &ldquo;not found&rdquo;. Dieser sollte aus SEO-Sicht unbedingt vermieden werden. Mit eben jenen erwähnten Weiterleitungen.</p>
<p>Um diese zu implementieren, gibt es freilich mehrere Möglichkeiten. Zunächst sollte man prüfen, welche Backlinks auf die eigene Seite überhaupt existieren. Je nachdem, wie viele Backlinks es gibt und wie die URL-Struktur der eigenen Seite beschaffen ist, bieten sich andere Möglichkeiten an. Man unterscheidet übrigens zwischen der temporären (HTTP-Statuscode 302) und der permanenten Weiterleitung(HTTP-Statuscode 302) . Die temporäre Weiterleitung kommt für unseren Fall nicht unbedingt infrage, da wir ja einen Umzug vor uns haben.</p>
<h2 id="weiterleitung-per-header-in-einer-php-datei">Weiterleitung per header() in einer PHP-Datei</h2>
<p>Eine ziemlich pragmatische und wenngleich effektive Methode ist es, die alte Ordner-Struktur per Hand nachzubilden und entsprechend PHP-Dateien anzulegen, die jeweils eine 301-Weiterleitung auf die neue URL enthalten (&quot;<a href="http://php.net/manual/de/function.header.php">header</a>&quot;). Das kann man machen, wird bei größeren System aber sehr schnell sehr aufwendig. Die PHP-Datei sollte unter dem Namen &ldquo;index.php&rdquo; in den jeweiligen Ordnern abgelegt werden und folgenden Code enthalten. Es folgt ein Beispiel für eine PHP-Datei befindet, die unter <em><a href="https://www.deine-seite.de/altes-ziel/index.php">https://www.deine-seite.de/altes-ziel/index.php</a></em> abgelegt ist. Diese enthält folgenden Code:</p>
<?php
   header("Location:https://www.deine-seite.de/neues-ziel/", true, 301); 
   exit;

Der erste Parameter verweist auf die neue Seite. Der zweite Parameter (_true_) legt lediglich fest, ob ein vorheriger Header ersetzt werden soll. Der letzte Parameter schließlich ist der HTTP-Statuscode, der übermittelt werden soll - also 301. Das exit am Schluss sorgt dafür, dass das PHP-Script am Ende auch wirklich abgeschlossen wird und dient hier nur als doppelter Boden. Ein schließendes ?> [wird nicht benötigt](https://developer.sugarcrm.com/2011/05/06/why-we-dont-using-closing-php-tags/).
<p>Das ist nicht nur mühsam, sondern auch nicht dynamisch. Die nächste Variante nutzt eine Weiterleitung mit .htaccess.</p>
<h2 id="weiterleitung-per-htaccess">Weiterleitung per .htaccess</h2>
<p>In einer .htaccess-Datei gibt es erstmal zwei Möglichkeiten, eine Weiterleitung einzurichten. Nutzt man einen regulären Ausdruck, wird entsprechend jeder passende eingehende Traffic abgefangen und an die gewünschte Hauptseite weitergeleitet.</p>
<p>Eine andere, etwas ausgefeiltere Möglichkeit ist es, die  alten Unterverzeichnisse bei Abfrage durch den Browser abzufangen und deren Namen als Suchparameter an das neue System weiterzuleiten.</p>
<p>Hier zum Beispiel der Unterordner &ldquo;foobar&rdquo;, der als Suchparameter genutzt werden soll:</p>
<p><a href="https://www.deine-seite.de/">www.deine-seite.de/</a><strong>foobar</strong></p>
<p>Der Name soll nun als Suchparameter folgendermaßen genutzt werden:</p>
<p><a href="https://www.deine-seite.de/index.php?search=">www.deine-seite.de/index.php?search=</a><strong>foobar</strong></p>
<p>So sieht die dafür notwendige .htaccess-Datei aus:</p>
<p>RewriteEngine On</p>
<p>RewriteRule ^(/?)([a-zA-Z0-9]+)?([\.html]+)?/?$ /index.php?search=$2 [R=301,NC]</p>
<p>RewriteRule ^(/?)([a-zA-Z0-9]+)?/([a-zA-Z0-9]+)?([\.html]+)?/?$ /index.php?search=$2\ $3 [L,R=301,NC]</p>
<p>Die erste Regel fängt Aufrufe der ersten Pfad-Ebene ab. Die zweite Regel kann auch die Namen der darunter liegenden Pfade abgreifen und daraus einen kombinierten Suchbegriff erzeugen. Das ist z.B. hilfreich, wenn man die Pfade eines Shop-Systems nutzen will um daraus im neuen Shop die Suche zu erzeugen.</p>
<p><a href="https://www.shop.de/foobar/hello">www.shop.de/foobar/hello</a>_world.html -&gt; <a href="https://www.shop.de/index.php?search=foobar%20hello">www.shop.de/index.php?search=foobar%20hello</a>_world</p>
<p>Die .htaccess-Datei muss sich im Unterordner /foobar befinden, der auch physisch vorhanden sein muss. Man könnte die Weiterleitung auch im Root-Verzeichnis anlegen und bräuchte dann den Unterordner nicht. So vermeidet man aber unnötigen Overhead, da die Regel nicht bei den regulären Aufrufen aktiviert wird.</p>

        
        
        <div class="tags">
          <p><strong>Tags:</strong> htaccess, redir, weiterleitung</p>
        </div>
        
      ]]></content:encoded>
      
      
      
      <category>hosting</category>
      
      <category>anleitungen</category>
      
      
      
      
      <media:content url="http://localhost:1313/images/posts/placeholder.jpg" type="image/jpeg">
        <media:title>Welche Möglichkeiten habe ich um Weiterleitungen anzulegen? - Titelbild</media:title>
      </media:content>
      
      
      
      
      <dc:subject>Lesezeit: 5 Minuten</dc:subject>
      
      
      
      <dc:type>guide</dc:type>
      
      
    </item>
  </channel>
</rss>